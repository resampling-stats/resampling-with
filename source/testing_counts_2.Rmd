---
resampling_with:
    ed2_fname: 21-Chap-17
jupyter:
  jupytext:
    notebook_metadata_filter: all,-language_info
    split_at_heading: true
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.14.6
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

```{r setup, include=FALSE}
source("_common.R")
```

# The Statistics of Hypothesis-Testing with Counted Data, Part 2

Here's the bad-news-good-news message again: The bad news is that the subject
of inferential statistics is extremely difficult — not because it is complex
but rather because it is subtle. The cause of the difficulty is that the world
around us is difficult to understand, and spoon-fed mathematical
simplifications which you manipulate mechanically simply mislead you into
thinking you understand that about which you have not got a clue.

The good news is that you — and that means *you* , even if you say you are "no
good at mathematics" — can understand these problems with a layperson's hard
thinking, even if you have no mathematical background beyond arithmetic and you
think that you have no mathematical capability. That's because the difficulty
lies in such matters as pin-pointing the right question, and understanding how
to interpret your results.

The problems in the previous chapter were tough enough. But this chapter
considers problems with additional complications, such as when there are
more than two groups, or paired comparisons for the same units of
observation.

But first, we need another addition to our {{< var lang >}} vocabulary.

## Logical operators {#sec-logical-operators}

```{python echo=FALSE}
# To make coins go as hoped.
_npr.default_rng = _make_default_seeded_rng(82)
```

```{r echo=FALSE}
# To make coins go as hoped.
set.seed(112)
```

::: {.notebook name="logical_operators" title="Logical operators"}

This section continues our programme of expanding the range of {{< var lang >}}
features that you can use to clear code.  As we introduce each feature, we will
use them in the following examples.

As motivation, we are about to do some simulations where we are interested in
the number of some particular type of observations in each trial.  For example,
let's do 10 coin tosses with {{< var sample >}}:

```{python}
import numpy as np
rnd = np.random.default_rng()
```

```{python}
# For each element, heads of tails is equally likely.
coins = rnd.choice(['heads', 'tails'], size=10)
coins
```

```{r}
# For each element, heads of tails is equally likely.
coins <- sample(c('heads', 'tails'), size=10, replace=TRUE)
coins
```

Let us now say that we are interested to record if the trial had *either* 2 or
fewer "heads" *or* two or fewer "tails".

We could write it like this:

```{python}
if np.sum(coins == 'heads') <= 2:
    print('Trial is of interest')
if np.sum(coins == 'tails') <= 2:
    print('Trial is of interest')
```

```{r}
if (sum(coins == 'heads') <= 2) {
    message('Trial is of interest')
}
if (sum(coins == 'tails') <= 2) {
    message('Trial is of interest')
}
```

It is a little repetitive to have to repeat the code identical code to print
the same message for either of the two cases, and it would be even more
repetitive if there were more lines of identical code to run for each of the
two cases.

{{< var lang >}} solves this problem with the
[`or`]{.python}[`|` (*or*)]{.r}
operator, like this:

```{python}
if np.sum(coins == 'heads') <= 2 or np.sum(coins == 'tails') <= 2:
    print('Trial is of interest')
```

```{r}
if (sum(coins == 'heads') <= 2 | sum(coins == 'tails') <= 2) {
    message('Trial is of interest')
}
```

:::{.callout-note}
## What is an operator? {#sec-what-is-an-operator}

Above, we called [`or`]{.python}[`|`]{.r} an *operator*. An operator, for our
purposes, is a special character [, or a word, ]{.python} that sits between two
values, and that tells {{< var lang >}} how to combine these values.

For example `+` is an operator.  When `+` sits between two numbers in code, {{<
var lang >}} interprets this to mean "take the two numbers on either side, and
make a new number that is the result of adding the two numbers":

```{python}
# + is an operator that, between two numbers, means "add the numbers".
1 + 3
```

```{r}
# + is an operator that, between two numbers, means "add the numbers".
1 + 3
```

`+`, `-`, `/` and `*` are all examples of operators that do *arithmetic* on the
numbers to either side — they are *arithmetic* operators.

```{python}
# * is an operator that, between two numbers, means "multiply the numbers".
2 * 4
```

```{r}
# * is an operator that, between two numbers, means "multiply the numbers".
2 * 4
```

:::
<!---
End of callout note.
-->

We are about to use the operator
[`or`]{.python}[`|` (*or*)]{.r}.
[`or`]{.python}[`|`]{.r} is a *logical* operator.  It is a logical operator
because it does not operate on *numbers* (as arithmetic operators do), but on
*logical* (*Boolean*) values — values that can be either {{< var true >}} or
{{< var false >}}.

For example, here we use
[`or`]{.python}[`|` (*or*)]{.r}.
to combine a {{< var true >}} value (on the left) with a {{< var false >}}
value (on the right).  It gives a result — {{< var true >}}.

```{python}
True or False
```

```{r}
# Read "|" here as "or".
TRUE | FALSE
```

[`or`]{.python}[`|` (*or*)]{.r} applies a very simple rule: if *either* the
left-hand (LH) *or* the right-hand (RH) values are {{< var true >}}, then
[`or`]{.python}[`|` (*or*)]{.r} evaluates to {{< var true >}}.  Only if
*neither* of the LH and RH values are {{< var true >}}, does it return {{< var
false >}}.

```{python}
# Both LH and RH are True, return True.
print('True or True result:', True or True)
# Only LH is True, return True.
print('True or False result:', True or False)
# Only RH is True, return True.
print('False or True result:', False or True)
# Neither LH nor RH are True, return False.
print('False or False result:', False or False)
```

```{r}
# Both LH and RH are TRUE, return TRUE.
message('TRUE | (or) TRUE result: ', TRUE | TRUE)
# Only LH is TRUE, return TRUE.
message('TRUE | (or) FALSE result: ', TRUE | FALSE)
# Only RH is TRUE, return TRUE.
message('FALSE | (or) TRUE result: ', FALSE | TRUE)
# Neither LH nor RH are TRUE, return FALSE.
message('FALSE | (or) FALSE result: ', FALSE | FALSE)
```

Now let's go back to the `if` statement above.  The conditional part of the
header line is:

```{python}
np.sum(coins == 'heads') <= 2 or np.sum(coins == 'tails') <= 2
```

```{r}
sum(coins == 'heads') <=2 | sum(coins == 'tails') <= 2
```

This will be {{< var true >}} *either* when there there are two or fewer
"heads", *or* when there are two or fewer tails.  Therefore, when we use this
conditional in an `if` statement, we make the *body* of the `if` statement run
only if either of the two conditions are {{< var true >}}.

```{python}
if np.sum(coins == 'heads') <= 2 or np.sum(coins == 'tails') <= 2:
    print('Trial is of interest')
```

```{r}
if (sum(coins == 'heads') <= 2 | sum(coins == 'tails') <= 2) {
    message('Trial is of interest')
}
```

While we are here, {{< var lang >}} has another very useful logical operator:
[`and`]{.python}['&' (*and*)]{.r}.

[`and`]{.python}['&']{.r} takes the LH and RH values, and returns
{{< var true >}} only if *both* values are {{< var true >}}.

```{python}
# Both LH and RH are True, return True.
print('True and True result:', True and True)
# Only LH is True, return False.
print('True and False result:', True and False)
# Only RH is True, return False.
print('False and True result:', False and True)
# Neither LH nor RH are True, return False.
print('False and False result:', False and False)
```

```{r}
# Both LH and RH are TRUE, return TRUE.
message('TRUE & (and) TRUE result: ', TRUE & TRUE)
# Only LH is TRUE, return FALSE.
message('TRUE & (and) FALSE result: ', TRUE & FALSE)
# Only RH is TRUE, return FALSE.
message('FALSE & (and) TRUE result: ', FALSE & TRUE)
# Neither LH nor RH are TRUE, return FALSE.
message('FALSE & (and) FALSE result: ', FALSE & FALSE)
```

We could, for example, ask whether the number of heads is >=3 *and* <=7 (is in
the range 3 through 7).

```{python}
if np.sum(coins == 'heads') >= 3 and np.sum(coins == 'heads') <= 7:
    print('Trial is of interest')
```

```{r}
if (sum(coins == 'heads') <= 3 | sum(coins == 'heads') <= 7) {
    message('Trial is of interest')
}
```

::: python
:::{.callout-note}
## Python interval comparison

In fact, Python has a special shortcut syntax called *interval comparison* for
that last question — whether a number is within a range.  It looks like this:

```{python}
# Asks whether thee number of heads is >= 3 *and* <= 7.
3 <= np.sum(coins == 'heads') <= 7
```

Notice the value at one end of the range the left (here, the lower value), then
the comparison operator, then the value to compare, then another comparison
operator, followed by the value at the other end of the range on the right.

The interval comparison above is a shortcut for the more verbose version we
would need when using `and`:

```{python}
# Also asks whether thee number of heads is >= 3 *and* <= 7.
3 <= np.sum(coins == 'heads') and np.sum(coins == 'heads') <= 7
```
:::
<!---
End of callout block.
-->
:::
<!---
End of Python block.
-->
:::
<!---
End of notebook.
-->

## Comparisons among more than two samples of counted data

### Example: Do Any of Four Treatments Affect Sex Ratio in Fruit Flies? {#sec-fruitfly-four}


This is an example of the general problem — **when the benchmark universe
proportion is known, is the proportion of the binomial population affected by
any of the treatments?**

Suppose that, instead of experimenting with just one type of radiation
treatment on the flies (as in @sec-fruitfly), you try *four* different
treatments, which we shall label A, B, C, and D. Treatment A produces fourteen
males and six females, but treatments B, C, and D produce ten, eleven, and ten
males, respectively. It is immediately obvious that there is no reason to think
that treatment B, C, or D affects the sex ratio. But what about treatment A?

A frequent and dangerous mistake made by young scientists is to scrounge around
in the data for the most extreme result, and then treat it as if it were the
only result. In the context of this example, it would be fallacious to think
that the probability of the fourteen-males-to-six females split observed for
treatment A is the same as the probability that we figured for a *single*
experiment in the example @sec-fruitfly. Instead, we must consider that our
benchmark universe is composed of *four sets* of twenty trials, each trial
having a 50-50 probability of being male. We can consider that our previous
trials 1-4 in @sec-fruitfly constitute a *single* new trial, and each
subsequent set of four previous trials constitute another new trial. We then
ask how likely a new trial of our sets of twenty flips is to produce *one* set
with fourteen or more of one or the other sex.

Let us make the procedure explicit, starting at the procedure from
@sec-fruitfly.  Again, we will check for 14 or more males, *or* 6 or fewer
males (meaning, 14 or more females).

* **Step 1.** Let tails = male, heads = female.
* **Step 2.** Flip twenty coins and count the number of tails (males).  Call
this the count for group A.  Repeat three more times to get counts for groups
B, C and D.
* **Step 5.** If *any* of the group counts for A, or B or C or D is 14 ore
more then record "Yes", or if *any* of the group counts A, B, C, D are 6 or
less (meaning >= 14 females), record "yes".   If neither is true, record "No".
* **Step 4.** Repeat steps 2 and 3 perhaps 100 times.
* **Step 5.** Calculate the proportion of "yes"  results in the 100 trials.
This proportion estimates the probability that a fruit fly population with
a proportion of 50 percent males will produce as many as 14 males, or as many
as 14 females, in at least one of four groups of 20 flies.

```{r echo=FALSE, eval=TRUE}
# Calculate some values for the text below.
flies4 <- read.csv('data/fruitfly_trials4.csv', check.names=FALSE)
n_trials <- nrow(flies4)
yeses <- flies4[['Any >=14 or any <=6']]
n_any_gte_14 <- sum(yeses == 'Yes')
# And from the original table.
flies <- read.csv('data/fruitfly_trials.csv', check.names=FALSE)
n_flies <- 25
fly_counts <- head(flies[['# of tails']], n_flies)
n_both <- sum(fly_counts >= 14 | fly_counts <= 6)
```

We begin the trials with data as in @tbl-fruitfly-4-groups.
In `r int2tw(n_any_gte_14)` of the `r int2tw(n_trials)` simulation trials, one
or more one samples (groups) shows 14 or more males. Without even concerning
ourselves about whether we should be looking at males or females, or just
males, or needing to do more trials, we can see that it would be *very common
indeed* to have one of four treatments show fourteen or more of one sex just
by chance. This discovery clearly indicates that a result that would be fairly
unusual (`r int2tw(n_both)` in `r int2tw(n_flies)`) for a single sample alone
is commonplace in one of four observed samples.

```{r echo=FALSE, eval=TRUE}
# Show the table.
ketable(head(flies4, n_trials),
        caption = "Results from 6 random trials for Fruitfly 4-group problem {#tbl-fruitfly-4-groups}")
```

::: {.notebook name="fruit_fly4" title="Fruit fly simulation of four groups"}

::: nb-only
This notebook uses simulation to test whether a result of 14 or more males, or 14 or more females, can frequently arise by chance, if we look at four groups at a time, in one trial.
:::

A key point of the notebook here is that each trial consists
of *four groups* of 20 randomly generated hypothetical fruit flies. And if we
consider 10,000 trials, we will be examining 40,000 sets of 20 fruit flies.

In each trial we generate 4 random samples (groups) of 20 fruit flies, and
for each, we count the number of males ("males"s) and then check whether
that group has more than 13 of either sex (actually, more than 13 "males"s
or less than 7 "males"). If it does, then we change a variable called
`unusual` to 1, which informs us that for this sample, at least 1 group
of 20 fruit flies had results as unusual as the results from the fruit flies
exposed to the four treatments.

After the 10,000 runs are made, we count the number of trials where one sample
had a group of fruit flies with 14 or more of either sex, and show the
results.

```{python}
import numpy as np
rnd = np.random.default_rng()

n_iters = 10_000

# Make array to store results for each trial.
results = np.zeros(n_iters)

for i in range(n_iters):
    # unusual indicates whether we have obtained any trial group with more
    # than 13 of either sex. We start at 0 (= no).
    unusual = 0
    # Repeat the following steps 4 times to constitute 4 trial groups
    # (representing treatments A, B, C, and D) of 20 flies each.
    for j in range(4):
        flies = rnd.choice(['male', 'female'], size=20)
        n_males = np.sum(flies == 'male')
        if n_males >= 14 or n_males <= 6:
            unusual = 1
    # unusual now tells us whether we got a result as extreme as that
    # observed (unusual == 1 if we did, unusual == 0 if not). We must
    # keep track of this result in the results variable, for each experiment.
    results[i] = unusual

# The number of trials for which at least one of the four tests
# exceeded 13 males or 13 females.
k = np.sum(results)
kk = k / n_iters

print('Proportion of trials with one or more group >=14 or <=6 :', kk)
```

```{r}
n_iters <- 10000

# Make vector to store results for each trial.
results <- numeric(n_iters)

for (i in 1:n_iters) {
    # unusual indicates whether we have obtained any trial group with more
    # than 13 of either sex. We start at 0 (= no).
    unusual <- 0
    # Repeat the following steps 4 times to constitute 4 trial groups
    # (representing treatments A, B, C, and D) of 20 flies each.
    for (j in 1:4) {
        flies <- sample(c('male', 'female'), replace=TRUE, size=20)
        n_males <- sum(flies == 'male')
        if (n_males >= 13 | n_males <= 6) {
            unusual <- 1
        }
    }
    # unusual now tells us whether we got a result as extreme as that
    # observed (unusual == 1 if we did, unusual == 0 if not). We must
    # keep track of this result in the results variable, for each experiment.
    results[i] <- unusual
}

# The number of trials for which at least one of the four tests
# exceeded 13 males or 13 females.
k <- sum(results)
kk <- k / n_iters

message('Proportion of trials with one or more group >=14 or <=6 :', kk)
```

:::
<!---
End of notebook
-->

In one set of 10,000 trials, there were more than 13 males or more than 13
females `r round(get_var('kk') * 100)` percent of the time — clearly not an
unusual occurrence.

### Example: Do Four Psychological Treatments Differ in Effectiveness? {#sec-four-rehabs}

**Do Several Two-Outcome Samples Differ Among Themselves in Their
Proportions?**

Consider four different psychological treatments designed to
rehabilitate young offenders. Instead of a numerical test score,
there is only a "yes" or a "no" answer as to whether the young person has
kept their record clean or has gotten into trouble again. Call a clean record
"success".  Label the treatments P, R, S, and T, each of which is administered
to a separate group of twenty young offenders. The number of "success"
outcomes per group has been: P, 17; R, 10; S, 10; T, 7. Is it improbable that
all four groups come from the same universe?

This problem is like the placebo vs. cancer-cure problem, but now there
are more than two samples. It is also like the four-sample
irradiated-fruit flies example (@sec-fruitfly-four), except that now we are
*not* asking whether any or some of the samples differ from a *given
universe* (50-50 sex ratio in that case). Rather, we are now asking
whether there are differences *among* the samples themselves. Please
keep in mind that we are still dealing with two-outcome (success-or-failure,
yes-or-no, well-or-sick) problems. The outcomes fall into categories, to which
we give labels ("success" or "failure", "well" or "sick").  Later we shall
take up problems that are similar except that the outcomes are "quantitative"
— in that the outcomes are numbers rather than labels.

If all four groups were drawn from the same universe, that universe has
an estimated success rate of 17/20 + 10/20 + 10/20 + 7/20 = 44/80
= 55/100 = 55%, because the observed data *taken as a whole* constitute our
best guess as to the nature of the universe from which they come — again, *if*
they all come from the same universe. (Please think this matter over a bit,
because it is important and subtle. It may help you to notice the absence of
any *other* information about the universe from which they have all come, if
they have come from the same universe.)

Therefore, select twenty two-digit numbers for each group from the
random-number table, marking "success" for each number in the range 1 through
55 and "failure" for each number 56 through 100. Conduct a number of such
trials. Then count the proportion of times that the difference between the
highest and lowest groups is larger than the widest observed difference, the
difference between P and T (17-7 = 10). In @tbl-offenders, none of the first
six trials shows anywhere near as large a difference as the observed range of
10, suggesting that it would be rare for four treatments that are "really"
similar to show so great a difference. There is thus reason to believe that
P and T differ in their effects.

| Trial     |  P    | R     | S     | T     | Largest Minus Smallest |
|-----------|-------|-------|-------|-------|------------------------|
| 1         | 11    | 9     | 8     | 12    | 4                      |
| 2         | 10    | 10    | 12    | 12    | 2                      |
| 3         | 9     | 12    | 8     | 12    | 4                      |
| 4         | 9     | 11    | 12    | 10    | 3                      |
| 5         | 10    | 10    | 11    | 12    | 1                      |
| 6         | 11    | 11    | 9     | 11    | 2                      |

: Results of Six Random Trials for Problem "offenders" {#tbl-offenders}

The strategy of the {{< var lang >}} solution to "offenders" is similar to the
strategy for previous problems in this chapter. The benchmark (null)
hypothesis is that the treatments do not differ in their effects observed, and
we estimate the probability that the observed results would occur by chance
using the benchmark universe. The only new twist is that we must instruct the
computer to find the groups with the highest and the lowest numbers of
success rates.

Using {{< var lang >}}, we generate four "treatments," each represented
by 20 labels.  We draw these 20 labels from the choices "success" or "failure", with a 55% chance of getting "success" and a 45% chance of "failure".  Follow along in the program for the rest of the procedure:

::: {.notebook name="offenders" title="Simulation of offender strategies"}

::: nb-only
This notebook uses simulation to test the null hypothesis that the four treatments in question were each equally effective.
:::

```{python}
import numpy as np
import matplotlib.pyplot as plt

# set up the random number generator
rnd = np.random.default_rng()

# Set the number of trials
n_trials = 10_000

# Set the sample size for each trial
sample_size = 20

# An empty array to store the trial results.
scores = np.zeros(n_trials)

# Do 10000 trials
for i in range(n_trials):
    # The first treatment group
    a = rnd.choice(['success', 'failure'], p=[0.55, 0.45], size=20)
    # Count successes.
    a_count = np.sum(a == 'success')
    # Second, third and fourth treatment groups, and successes.
    b = rnd.choice(['success', 'failure'], p=[0.55, 0.45], size=20)
    b_count = np.sum(b == 'success')
    c = rnd.choice(['success', 'failure'], p=[0.55, 0.45], size=20)
    c_count = np.sum(c == 'success')
    d = rnd.choice(['success', 'failure'], p=[0.55, 0.45], size=20)
    d_count = np.sum(d == 'success')

    # Now find all the pairwise differences in successes among the groups.
    a_minus_b = a_count - b_count
    a_minus_c = a_count - c_count
    a_minus_d = a_count - d_count
    b_minus_c = b_count - c_count
    b_minus_d = b_count - d_count
    c_minus_d = c_count - d_count

    # Concatenate, or join, all the differences in a single array "diffs".
    diffs = np.array([a_minus_b, a_minus_c, a_minus_d,
                      b_minus_c, b_minus_d, c_minus_d])
    # Since we are interested only in the magnitude of the difference, not its
    # direction, we take the absolute value of all the differences (we remove
    # any minus signs, making all values positive).
    abs_diffs = np.abs(diffs)
    # Find the largest of all the differences
    max_abs_diff = np.max(abs_diffs)
    # Keep score of the largest
    scores[i] = max_abs_diff

    # End a trial, go back and repeat until all 10000 are complete.

# How many of the trials yielded a maximum difference greater than the
# observed maximum difference?
k = np.sum(scores >= 10)
# Convert to a proportion
kk = k / n_trials

print('Proportion >= 10 was', kk)
```

```{r}
# Set the number of trials
n_trials <- 10000

# Set the sample size for each trial
sample_size <- 20

# An empty array to store the trials
scores <- numeric(n_trials)

# Do 10000 trials
for (i in 1:n_trials) {
    # The first treatment group
    a <- sample(c('success', 'failure'),
                size=sample_size,
                prob=c(0.55, 0.45),
                replace=TRUE)
    # Count successes.
    a_count <- sum(a == 'success')
    # Second, third and fourth treatment groups, and successes.
    b <- sample(c('success', 'failure'),
                size=sample_size,
                prob=c(0.55, 0.45),
                replace=TRUE)
    b_count <- sum(b == 'success')
    c <- sample(c('success', 'failure'),
                size=sample_size,
                prob=c(0.55, 0.45),
                replace=TRUE)
    c_count <- sum(c == 'success')
    d <- sample(c('success', 'failure'),
                size=sample_size,
                prob=c(0.55, 0.45),
                replace=TRUE)
    d_count <- sum(d == 'success')

    # Now find all the pairwise differences in successes among the groups.
    a_minus_b <- a_count - b_count
    a_minus_c <- a_count - c_count
    a_minus_d <- a_count - d_count
    b_minus_c <- b_count - c_count
    b_minus_d <- b_count - d_count
    c_minus_d <- c_count - d_count

    # Concatenate, or join, all the differences in a single array "diffs".
    diffs <- c(a_minus_b, a_minus_c, a_minus_d,
               b_minus_c, b_minus_d, c_minus_d)
    # Since we are interested only in the magnitude of the difference, not its
    # direction, we take the absolute value of all the differences (we remove
    # any minus signs, making all values positive).
    abs_diffs <- abs(diffs)
    # Find the largest of all the differences
    max_abs_diff <- max(abs_diffs)
    # Keep score of the largest
    scores[i] <- max_abs_diff

    # End a trial, go back and repeat until all 10000 are complete.
}

# How many of the trials yielded a maximum difference greater than the
# observed maximum difference?
k <- sum(scores >= 10)
# Convert to a proportion
kk <- k / n_trials

message('Proportion >= 10 was: ', kk)
```

:::
<!---
End of notebook
-->

Only `r int2tw(round(get_var('kk') * 100))` percent of the experiments with
randomly generated treatments from a common success rate of .55 produced
differences in excess of the observed maximum difference (10).

An alternative approach to this problem would be to deal with *each* result's
departure from the mean, rather than the largest difference among the pairs.
Once again, we want to deal with *absolute* departures, since we are interested
only in magnitude of difference. We could take the absolute value of the
differences, as above, but we will try something different here. *Squaring* the
differences also renders them all positive: this is a common approach in
statistics.

::: {.notebook name="offenders_squared" title="Simulation of offender strategies using squared differences"}

::: nb-only
This notebook uses squared differences in a simulation to test the null
hypothesis that the four treatments in question were each equally effective.
:::

The first step is to examine our data and calculate this measure: The
mean is 11, the differences (call *deviations from the mean*) are 6, 1, 1, and
4, the squared deviations are 36, 1, 1, and 16, and their sum is 54.  In {{<
var lang >}}:

```{python}
# The actual scores for each treatment.
treat_scores = np.array([17, 10, 10, 7])
# The mean.
m = np.mean(treat_scores)
# The four deviations from the mean
deviations = treat_scores - m
# Squared deviations.
sq_deviations = deviations ** 2
# Sum of squared deviations.
actual_sum_sq_deviations = np.sum(sq_deviations)
# Show the result.
actual_sum_sq_deviations
```

```{r}
# The actual scores for each treatment.
treat_scores <- c(17, 10, 10, 7)
# The mean.
m <- mean(treat_scores)
# The four deviations from the mean
deviations <- treat_scores - m
# Squared deviations.
sq_deviations <- deviations ** 2
# Sum of squared deviations.
actual_sum_sq_deviations <- sum(sq_deviations)
# Show the result.
actual_sum_sq_deviations
```

Our experiment will be, as before, to constitute four groups of 20 at random
from a universe with a 55 percent rehabilitation rate. We then calculate this
same measure for the random groups. If it is frequently larger than 54, then we
conclude that a uniform cure rate of 55 percent could easily have produced the
observed results. The program that follows also generates the four treatments
by using a `for` loop, rather than spelling out the {{< var sample >}} command
4 times as above. In {{< var lang >}}:

```{python}
import numpy as np
import matplotlib.pyplot as plt

# Set up the random number generator.
rnd = np.random.default_rng()

# Set the number of trials.
n_trials = 10000

# Set the sample size for each trial.
sample_size = 20

# An empty array to store the trials.
scores = np.zeros(n_trials)

# Do 10000 trials
for i in range(n_trials):
    # Repeat the following steps 4 times to constitute 4 groups of 20 and
    # count their success rates.
    group_scores = np.zeros(4)
    for group_no in range(4):  # for 0, 1, 2, 3.
        # A treatment group
        group = rnd.choice(['success', 'failure'], p=[0.55, 0.45], size=20)
        # Count successes.
        group_count = np.sum(group == 'success')
        # Store result
        group_scores[group_no] = group_count
        # End the procedure for one group of 20, go back and repeat until all 4
        # are done.
    # Calculate the mean
    m = np.mean(group_scores)
    # Calculate the deviations of the scores from the mean of the scores.
    deviations = group_scores - m
    # Square them, making them them positive.
    sq_deviations = deviations ** 2
    # Sum up the squared deviations.
    sum_sq_deviations = np.sum(sq_deviations)
    # Keep track of the result for each trial.
    scores[i] = sum_sq_deviations

    # End a trial, go back and repeat until all 10000 are complete.

# Produce a histogram of the trial results.
plt.hist(scores, bins=50)
plt.title('Distribution of sum of squared deviations, in null universe')
```

```{r}
# Set the number of trials.
n_trials <- 10000

# Set the sample size for each trial.
sample_size <- 20

# An empty vector to store the trials.
scores <- numeric(n_trials)

# Do 10000 trials.
for (i in 1:n_trials) {
    # Repeat the following steps 4 times to constitute 4 groups of 20 and
    # count their success rates.
    group_scores <- numeric(4)
    for (group_no in 1:4) {
        # A treatment group
        group <- sample(c('success', 'failure'),
                        size=20,
                        prob=c(0.55, 0.45),
                        replace=TRUE)
        # Count successes.
        group_count <- sum(group == 'success')
        # Store result.
        group_scores[group_no] <- group_count
        # End the procedure for one group of 20, go back and repeat until all 4
        # are done.
    }
    # Calculate the mean
    m <- mean(group_scores)
    # Calculate the deviations of the scores from the mean of the scores.
    deviations <- group_scores - m
    # Square them, making them them positive.
    sq_deviations <- deviations ** 2
    # Sum up the squared deviations.
    sum_sq_deviations <- sum(sq_deviations)
    # Keep track of the result for each trial.
    scores[i] <- sum_sq_deviations

    # End a trial, go back and repeat until all 10000 are complete.
}

# Produce a histogram of the trial results.
hist(scores, breaks=50,
     main='Distribution of sum of squared deviations, in null universe')
```

From this histogram, we see that in only about 1 percent of the cases did our
trial sum of squared differences equal or exceed 54, confirming our conclusion
that this is an unusual result. We can have {{< var lang >}} calculate this
proportion:

```{python}
# How many of the trials yielded a maximum difference greater than the
# observed maximum difference?
k = np.sum(scores >= actual_sum_sq_deviations)
# Convert to a proportion
kk = k / n_trials

print('Proportion >=', actual_sum_sq_deviations, 'was:', kk)
```

```{r}
# How many of the trials yielded a maximum difference greater than the
# observed maximum difference?
k <- sum(scores >= actual_sum_sq_deviations)
# Convert to a proportion
kk <- k / n_trials

message('Proportion >= ', actual_sum_sq_deviations, ' was: ', kk)
```

The conventional way to approach this problem would be with what is known as
a [chi-squared test](https://en.wikipedia.org/wiki/Chi-squared_test).

:::
<!---
End of notebook
-->

### Example: Three-way Comparison

In a national election poll of 750 respondents in May, 1992, George Bush got 36
percent of the preferences (270 voters), Ross Perot got 30 percent (225
voters), and Bill Clinton got 28 percent (210 voters), with 45 undecided (*Wall
Street Journal*, October 29, 1992, A16). Assuming that the poll was
representative of actual voting, how likely is it that Bush was actually
*behind* and just came out ahead in this poll by chance? Or to put it
differently, what was the probability that Bush actually had a plurality of
support, rather than that his apparent advantage was a matter of sampling
variability?  We test this by constructing a universe in which Bush is slightly
behind (in practice, just equal), and then drawing samples to see how likely it
is that those samples will show Bush ahead.

We must first find that universe — among all possible universes that yield
a conclusion contrary to the conclusion shown by the data, and one in which we
are interested — that has the highest probability of producing the observed
sample. With a two-person race the universe is obvious: a universe that is
evenly split except for a single vote against "our" candidate who is now in the
lead, i.e. in practice a 50-50 universe. In that simple case we then ask the
probability that *that* universe would produce a sample as far out in the
direction of the conclusion drawn from the observed sample as the observed
sample.

With a three-person race, however, the decision is not obvious (and if this
problem becomes too murky for you, skip over it; it is included here more for
fun than anything else). And there is no standard method for handling this
problem in conventional statistics (a solution in terms of a confidence
interval was first offered in 1992, and that one is very complicated and not
very satisfactory to me (JLS)). But the sort of thinking that we must labor to
accomplish is also required for any conventional solution; the difficulty is
inherent in the problem, rather than being inherent in resampling, and
resampling will be at least as simple and understandable as any formulaic
approach.

Before we start to think about this problem, let us simplify by ignoring the 45
undecided voters, and adjusting the poll percentages accordingly.  Of the
remaining voters, Bush got 270 / 705 = 38.3%, Perot had 225 / 705 = 31.9%, and
Clinton had 210 / 705 = 29.8%.  Bush's lead over Perot, in voters with
a declared preference, was therefore 38.3 - 31.9 = 6.4%.

The relevant universe is (or so I think) a universe that is 35 Bush — 35 Perot
— 30 Clinton (for a race where the poll indicates a 38.3-31.9-29.8% split); the
35-35-30 universe is of interest because it is the universe that is closest to
the observed sample that does not provide a win for Bush; it is roughly
analogous to the 50-50 split in the two-person race, though a clear-cut
argument would require a lot more discussion. A universe that is split
34-34-32, or any of the other possible universes, is less likely to produce
a 36-30-28 sample (such as was observed) than is a 35-35-30
universe.[^max-like-votes]. (In technical terms, it might be a "maximum
likelihood universe" that we are looking for.) For completeness, we might also
try a 36-36-28 universe to see if that produces a result very different than
the 35-35-30 universe.

[^max-like-votes]: We are interested in choosing the benchmark universe that a)
    has Bush with equal or lower percentage votes than another candidate and b)
    has the largest probability of giving rise to the observed vote proportions
    in the sample.  We can show by trying all possible options that the 35, 35,
    30 universe has the highest probability in giving rise to the (38.3%,
    31.9%, 29.8%) sample percentages (270, 225, 210 vote counts).  See [this
    Python
    notebook](https://github.com/resampling-stats/book-notes/blob/main/maximum_likelihood_for_election.ipynb)
    for the simulation.

Among those universes where Bush is behind (or equal), a universe that is split
50-50-0 (with just one extra vote for the closest opponent to Bush) would be
the most likely to produce a 6 percent difference between the top two
candidates by chance, but we are not prepared to believe that the voters are
split in such a fashion. This assumption shows that we are bringing some
judgments to bear from outside the observed data.

For now, the point is not *how* to discover the appropriate benchmark
hypothesis, but rather its *criterion* — which is, I repeat, that universe
(among all possible universes) that yields a conclusion contrary to the
conclusion shown by the data (and in which we are interested) and that (among
such universes that yield such a conclusion) has the highest probability of
producing the observed sample.

Let's go through the logic again: 1) Bush apparently has a 6.4% percent lead
over the second-place candidate. 2) We ask if the second-place candidate might
be ahead if *all voters* were polled, rather than just this sample. We test
that by setting up a universe in which the second-place candidate is
infinitesimally ahead (in practice, we make the two top candidates equal in our
hypothetical universe). And we make the third-place candidate somewhere close
to the top two candidates. 3) We then draw samples from this universe and
observe how often the result is a 6.4% percent lead for the top candidate (who
starts off just below or equal in the universe).

From here on, the procedure is straightforward: Determine how likely that
universe is to produce a sample as far (or further) away in the direction of
"our" candidate winning. (One could do something like this even if the
candidate of interest were not now in the lead.)

This problem teaches again that one must think explicitly about the choice of
a benchmark hypothesis. The grounds for the choice of the benchmark hypothesis
should precede the notebook, or should be included as an extended commentary within the notebook.

This {{< var lang >}} code embodies the previous line of thought.

::: {.notebook name="bush_clinton" title="Simulation of Bush / Clinton polling"}

::: nb-only
The notebook estimates the chances that Bush is in fact equal or behind in the population of eventual voters, despite the poll giving him a narrow lead.
:::

```{python}
import numpy as np
import matplotlib.pyplot as plt

# set up the random number generator
rnd = np.random.default_rng()

# Set the number of trials.
n_trials = 10000

# Number of voters who expressed a preference.
n_voters = 705

# Benchmark proportions.
bench_ps = [0.35, 0.35, 0.30]

# An empty array to store the trials.
scores = np.zeros(n_trials)

# Do 10000 trials
for i in range(n_trials):
    # Take a sample of 705 votes, with replacement.
    samp = rnd.choice(['Bush', 'Perot', 'Clinton'],
                      p=bench_ps,
                      size=n_voters)
    # Count the Bush voters, etc.
    n_bush = np.sum(samp == 'Bush')
    n_perot = np.sum(samp == 'Perot')
    n_clinton = np.sum(samp == 'Clinton')
    # Join Perot & Clinton votes in an array.
    others = np.array([n_perot, n_clinton])
    # Find the larger of the other two.
    n_second = np.max(others)
    # Find Bush's margin over 2nd.
    lead = n_bush - n_second
    # Convert vote difference to percent lead.
    pct_lead = lead / n_voters * 100
    # Store the result.
    scores[i] = pct_lead

plt.hist(scores, bins=50)
plt.title('Distribution of Bush margin over second candidate')

# Compare to the observed margin in the sample of 705 corresponding to a 6.4
# percent margin by Bush over 2nd place finisher.
k = np.sum(scores >= 6.4)
kk = k / n_trials

print('Proportion of trials where Bush margin >= 6.4%:', kk)
```

```{r}
# Number of voters who expressed a preference.
n_voters <- 705

# Benchmark proportions.
bench_ps <- c(0.35, 0.35, 0.30)

# Set the number of trials.
n_trials <- 10000

# An empty array to store the trials.
scores <- numeric(n_trials)

# Do 10000 trials
for (i in 1:n_trials) {
    # Take a sample of 705 votes, with replacement.
    samp <- sample(c('Bush', 'Perot', 'Clinton'),
                   size=n_voters,
                   prob=bench_ps,
                   replace=TRUE)
    # Count the Bush voters, etc.
    n_bush <- sum(samp == 'Bush')
    n_perot <- sum(samp == 'Perot')
    n_clinton <- sum(samp == 'Clinton')
    # Join Perot & Clinton votes into a vector.
    others <- c(n_perot, n_clinton)
    # Find the larger of the other two.
    n_second <- max(others)
    # Find Bush's margin over 2nd.
    lead <- n_bush - n_second
    # Convert vote difference to percent lead.
    pct_lead <- lead / n_voters * 100
    # Store the result.
    scores[i] <- pct_lead
}

hist(scores, breaks=50,
     main='Distribution of Bush margin over second candidate')

# Compare to the observed margin in the sample of 705 corresponding to a 6.4
# percent margin by Bush over 2nd place finisher (rounded).
k <- sum(scores >= 6.4)
kk <- k / n_trials

message('Proportion of trials where Bush margin >= 6.4%: ', kk)
```

:::
<!---
End of notebook
-->

When we run this program with a 36-36-28 split, we also get a similar result
— around 2.2 percent (try it — edit `bench_ps` in the notebook and run it
again).

Our main result is that our 35-35-30 analysis shows a probability of only
`r round(get_var('kk') * 100, 1)` percent that Bush would score a 6.4
percentage point "victory" in the sample, by chance, if the universe were split
as specified. So Bush could feel reasonably confident that at the time the poll
was taken, he was ahead of the other two candidates.

## Paired Comparisons With Counted Data

### Example: the pig rations again, but comparing pairs of pigs {#sec-pig-pairs}

This is a **Paired-Comparison Test**.

To illustrate how several different procedures can reasonably be used to
deal with a given problem, here is another way to decide whether pig
ration A is "really" better: We can assume that the order of the pig
scores listed *within each ration group* is random — perhaps the order
of the stalls the pigs were kept in, or their alphabetical-name order,
or any other random order *not related to their weights* . Match the
first pig eating ration A with the first pig eating ration B, and also
match the second pigs, the third pigs, and so forth. Then count the
number of matched pairs on which ration A does better. On nine of twelve
pairings ration A does better, that is, 31.0 \> 26.0, 34.0 \> 24.0, and
so forth.

Now we can ask: *If* the two rations are equally good, how often will
one ration exceed the other nine or more times out of twelve, just by
chance? This is the same as asking how often either heads *or* tails
will come up nine or more times in twelve tosses. (This is a
"two-tailed" test because, as far as we know, either ration may be as
good as or better than the other.) Once we have decided to treat the
problem in this manner, it is quite similar to @sec-fruitfly (the first
fruitfly irradiation problem). We ask how likely it is that the outcome will be
as far away as the observed outcome (9 "heads" of 12) from 6 of 12 (which is
what we expect to get by chance in this case if the two rations are similar).

So we conduct perhaps fifty trials as in @tbl-pigs-pair, where an asterisk
denotes nine or more heads or tails.

* **Step 1.** Let odd numbers equal "A better" and even numbers equal "B
  better."
* **Step 2.** Examine 12 random digits and check whether 9 or more, or 3 or
less, are odd. If so, record "yes," otherwise "no."
* **Step 3.** Repeat step 2 fifty times.
* **Step 4.** Compute the proportion "yes," which estimates the probability
sought.

The results are shown in @tbl-pigs-pair.

In 8 of 50 simulation trials, one or the other ration had nine or more tosses
in its favor. Therefore, we estimate the probability to be .16 (eight of fifty)
that samples this different would be generated by chance if the samples came
from the same universe.

+-----------+-----------+-----------+-----------+-----------+-----------+
|   Trial   | "Heads"   | "Tails"   |   Trial   | "Heads"   | "Tails"   |
|           | or        | or        |           | or        | or        |
|           | "Odds"    | "Evens"   |           | "Odds"    | "Evens"   |
|           +-----------+-----------+           +-----------+-----------+
|           | Ration A  | Ration B  |           | Ration A  | Ration B  |
+===========+===========+===========+===========+===========+===========+
| 1         | 6         | 6         | 26        | 6         | 6         |
+-----------+-----------+-----------+-----------+-----------+-----------+
| 2         | 4         | 8         | 27        | 5         | 7         |
+-----------+-----------+-----------+-----------+-----------+-----------+
| 3         | 6         | 6         | 28        | 7         | 5         |
+-----------+-----------+-----------+-----------+-----------+-----------+
| 4         | 7         | 5         | 29        | 4         | 8         |
+-----------+-----------+-----------+-----------+-----------+-----------+
| **5**     | 3         | 9         | 30        | 6         | 6         |
+-----------+-----------+-----------+-----------+-----------+-----------+
| 6         | 5         | 7         | **31**    | 9         | 3         |
+-----------+-----------+-----------+-----------+-----------+-----------+
| 7         | 8         | 4         | **32**    | 2         | 10        |
+-----------+-----------+-----------+-----------+-----------+-----------+
| 8         | 6         | 6         | 33        | 7         | 5         |
+-----------+-----------+-----------+-----------+-----------+-----------+
| 9         | 7         | 5         | 34        | 5         | 7         |
+-----------+-----------+-----------+-----------+-----------+-----------+
| **10**    | 9         | 3         | 35        | 6         | 6         |
+-----------+-----------+-----------+-----------+-----------+-----------+
| 11        | 7         | 5         | 36        | 8         | 4         |
+-----------+-----------+-----------+-----------+-----------+-----------+
| **12**    | 3         | 9         | 37        | 6         | 6         |
+-----------+-----------+-----------+-----------+-----------+-----------+
| 13        | 5         | 7         | 38        | 4         | 8         |
+-----------+-----------+-----------+-----------+-----------+-----------+
| 14        | 6         | 6         | 39        | 5         | 7         |
+-----------+-----------+-----------+-----------+-----------+-----------+
| 15        | 6         | 6         | 40        | 8         | 4         |
+-----------+-----------+-----------+-----------+-----------+-----------+
| 16        | 8         | 4         | 41        | 5         | 7         |
+-----------+-----------+-----------+-----------+-----------+-----------+
| 17        | 5         | 7         | 42        | 6         | 6         |
+-----------+-----------+-----------+-----------+-----------+-----------+
| **18**    | 9         | 3         | 43        | 5         | 7         |
+-----------+-----------+-----------+-----------+-----------+-----------+
| 19        | 6         | 6         | 44        | 7         | 5         |
+-----------+-----------+-----------+-----------+-----------+-----------+
| 20        | 7         | 5         | 45        | 6         | 6         |
+-----------+-----------+-----------+-----------+-----------+-----------+
| 21        | 4         | 8         | 46        | 4         | 8         |
+-----------+-----------+-----------+-----------+-----------+-----------+
| **22**    | 10        | 2         | 47        | 5         | 7         |
+-----------+-----------+-----------+-----------+-----------+-----------+
| 23        | 6         | 6         | 48        | 5         | 7         |
+-----------+-----------+-----------+-----------+-----------+-----------+
| 24        | 5         | 7         | 49        | 8         | 4         |
+-----------+-----------+-----------+-----------+-----------+-----------+
| **25**    | 3         | 9         | 50        | 7         | 5         |
+-----------+-----------+-----------+-----------+-----------+-----------+

: Results from fifty simulation trials of the paired pigs problem {#tbl-pigs-pair}

Now for a {{< var lang >}} program and results. This notebook is different from
the example at @sec-pig-rations in that it compares the weight-gain results of
*pairs* of pigs, instead of simply looking at the *rankings* for weight gains.

The key to the pigs pair notebook is the {{< var sample >}} line. If we assume
that ration A does not have an effect on weight gain (which is the "benchmark"
or "null" hypothesis), then the results of the actual experiment would be no
different than if we randomly {{< var sample >}} "A" and "B" and treat
an "A" as a larger weight gain for the ration A pig, and a "B" as a larger
weight gain for the ration B pig. Both events have a .5 chance of occurring for
each pair of pigs because if the rations had no effect on weight gain (the null
hypothesis), ration A pigs would have larger weight gains about half of the
time. The next step is to count (`sum`) the number of times that the weight
gains of one group (call it the group fed with ration A) were larger than the
weight gains of the other (call it the group fed with ration B). The complete
program follows:

::: {.notebook name="pig_pairs" title="Paired test for pig rations"}

::: nb-only
Use simulation to test for a difference in pig rations, by pairing pigs and testing the probability of observed differences within the pairs.
:::


```{python}
import numpy as np
rnd = np.random.default_rng()

n_iters = 10_000

# Make array to store results for each trial.
results = np.zeros(n_iters)

# Do 10000 trials
for i in range(n_iters):
    # Generate randomly 12 "A" or "B"s.  Each of the 12 values represents
    # one "pairing" where "A" = ration A "wins," "B" = ration B "wins.".
    wins = rnd.choice(['A', 'B'], size=12)
    # Count the number of "pairings" where ration A won, put the result in
    # "n_a_wins".
    n_a_wins = np.sum(wins == 'A')
    # Keep track of the result in results.
    results[i] = n_a_wins
    # End the trial, go back and repeat until all 10000 trials are complete.
# Determine how often we got 9 or more "wins" for ration A.
high = np.sum(results >= 9)
# Determine how often we got 3 or fewer "wins" for ration A.
low = np.sum(results <= 3)
# Add the two together
k = high + low
#  Convert to a proportion
kk = k / n_iters
# Print the result.
print('Proportion >=9 or <=3:', kk)
```

```{r}
n_iters <- 10000

# Make array to store results for each trial.
results <- numeric(n_iters)

# Do 10000 trials
for (i in 1:n_iters) {
    # Generate randomly 12 "A" or "B"s.  Each of the 12 values represents
    # one "pairing" where "A" <- ration A "wins," "B" <- ration B "wins.".
    wins <- sample(c('A', 'B'), replace=TRUE, size=12)
    # Count the number of "pairings" where ration A won, put the result in
    # "n_a_wins".
    n_a_wins <- sum(wins == 'A')
    # Keep track of the result in results.
    results[i] <- n_a_wins
    # End the trial, go back and repeat until all 10000 trials are complete.
}
# Determine how often we got 9 or more "wins" for ration A.
high <- sum(results >= 9)
# Determine how often we got 3 or fewer "wins" for ration A.
low <- sum(results <= 3)
# Add the two together
k <- high + low
#  Convert to a proportion
kk <- k / n_iters
# Print the result.
message('Proportion >=9 or <=3: ', kk)
```

:::
<!---
End of notebook
-->

Notice how we proceeded in examples @sec-pig-rations and @sec-pig-pairs. The
data were originally quantitative — weight gains in pounds for each pig. But
for simplicity we classified the data into simpler counted-data formats. The
first format (@sec-pig-rations) was a rank order, from highest to lowest. The
second format (@sec-pig-pairs) was simply higher-lower, obtained by randomly
pairing the observations (using alphabetical letter, or pig's stall number, or
whatever was the cause of the order in which the data were presented to be
random). Classifying the data in either of these ways loses some information
and makes the subsequent tests somewhat cruder than more refined analysis could
provide (as we shall see in the following chapter), but the loss of efficiency
is not crucial in many such cases. We shall see how to deal directly with the
quantitative data in @sec-testing-measured.

### Example: merged firms compared to two non-merged groups

<!---
Example 17-5
-->

In a study by Simon, Mokhtari, and Simon [-@simon1996mergers], a set of 33
advertising agencies that merged over a period of years were each compared to
entities within two groups (each also of 33 firms) that did not merge; one
non-merging group contained firms of roughly the same size as the final merged
entities, and the other non-merging group contained pairs of non-merging firms
whose total size was roughly the same as the total size of the merging
entities.

The idea behind the matching was that each pair of merged firms was
compared against

1.  a pair of contemporaneous firms that were roughly the same size as
    the merging firms *before* the merger, and
2.  a single firm that was roughly the same size as the merged entity
    *after* the merger.

Here (@tbl-mergers-data) are the data (provided by the authors):

```{r echo=FALSE, eval=TRUE}
# Show the table.
mergers <- read.csv('data/mergers.csv', check.names=FALSE)
ketable(mergers,
        caption ="Revenue Growth In Year 1 Following Merger
        {#tbl-mergers-data}")
```

Comparisons were made in several years before and after the mergings
to see whether the merged entities did better or worse than the
non-merging entities they were matched with by the researchers, but
for simplicity we may focus on just one of the more important years
in which they were compared — say, the revenue growth rates in the
year after the merger.

Here are those average revenue growth rates for the three groups:

```{r echo=FALSE, eval=TRUE}
merged_means <- as.data.frame(colMeans(mergers[,-1]))
names(merged_means) <- c('Mean revenue growth')
ketable(merged_means,
        caption ="Year's revenue growth {#tbl-merger-means}")
```

We could do a general test to determine whether there are differences
among the means of the three groups, as was done in the "Differences Among
4 Pig Rations" problem (@sec-pig-rations-measured). However, we note that
there may be considerable variation from one matched set to another —
variation which can obscure the overall results if we resample from a
large general bucket.

Therefore, we use the following resampling procedure that maintains the
separation between matched sets by converting each observation into a rank (1,
2 or 3) within the matched set.

Here (@tbl-merger-ranks) are those ranks, where 1 = worst, 3 = best:

```{r echo=FALSE, eval=TRUE}
# Show the table.
merger_ranks <- read.csv('data/merger_ranks.csv', check.names=FALSE)
ketable(merger_ranks,
        caption ="Revenue growth ranked within matched set (1=best)
        {#tbl-merger-ranks}")
```

These (@tbl-merger-mean-ranks) are the average ranks for the three groups (1
= worst, 3 = best):

```{r echo=FALSE, eval=TRUE}
merged_rank_means <- as.data.frame(colMeans(merger_ranks[,-1]))
names(merged_rank_means) <- c('Mean rank')
actual_mean_rank <- merged_rank_means[1,1]
ketable(merged_rank_means,
        caption ="Mean rank of year revenue growth {#tbl-merger-mean-ranks}")
```

Is it possible that the merged group received such a low (poor)
average ranking just by chance? The null hypothesis is that the
ranks within each set were assigned randomly, and that "merged" came
out so poorly just by chance. The following procedure simulates
random assignment of ranks to the "merged" group:

1.  Randomly select 33 integers between "1" and "3" (inclusive).
2.  Find the average rank & record.
3.  Repeat steps 1 and 2, say, 10,000 times.
4.  Find out how often the average rank is as low as
    `r round(actual_mean_rank, 2)`.

Here's a {{< var lang >}} notebook to apply those steps:

::: {.notebook name="merger_ranks" title="Merger rank test"}

::: nb-only
Test whether observed mean rank is plausible in the null-world where ranks are randomly drawn from 1 through 3.
:::

```{python}
import numpy as np
import matplotlib.pyplot as plt

rnd = np.random.default_rng()

n_iters = 10_000

# Make array to store results for each trial.
results = np.zeros(n_iters)

for i in range(n_iters):
    ranks = rnd.choice([1, 2, 3], size=33)
    mean_rank = np.mean(ranks)
    results[i] = mean_rank

plt.hist(results, bins=20)
plt.title('Mean ranks from random rank sampling')

k = np.sum(results <= 1.48)
kk = k / n_iters

print('Proportion of mean ranks <= 1.48:', kk)
```

```{r}
n_iters <- 10000

# Make array to store results for each trial.
results <- numeric(n_iters)

for (i in 1:n_iters) {
    ranks <- sample(c(1, 2, 3), size=33, replace=TRUE)
    mean_rank <- mean(ranks)
    results[i] <- mean_rank
}

hist(results, breaks=20,
     main='Mean ranks from random rank sampling')

k <- sum(results <= 1.48)
kk <- k / n_iters

message('Proportion of mean ranks <= 1.48: ', kk)
```

:::
<!---
End of notebook
-->

Interpretation: 10000 random selections of 33 ranks produced an
average as low as the observed average in approximately
`r round(get_var('kk') * 100, 1)` percent of the trials. Therefore we rule out
chance as an explanation for the poor ranking of the merged firms.

Exactly the same technique might be used in experimental medical studies
wherein subjects in an experimental group are matched with two different
entities that receive placebos or control treatments.

For example, there have been several recent three-way tests of treatments for
depression: drug therapy versus cognitive therapy versus combined drug and
cognitive therapy. If we are interested in the combined drug-therapy treatment
in particular, comparing it to standard existing treatments, we can proceed in
the same fashion as in the merger problem.

We might just as well consider the real data from the merger as
hypothetical data for a proposed test in 33 triplets of people that have
been matched within triplet by sex, age, and years of education. The
three treatments were to be chosen randomly within each triplet.

Assume that we now switch scales from the merger data, so that \#1 =
best and \#3 = worst, and that the outcomes on a series of tests were
ranked from best (\#1) to worst (\#3) within each triplet. Assume that
the combined drug-and-therapy regime has the best average rank. How sure
can we be that the observed result would not occur by chance? Here are
the data from the merger study, seen here as @tbl-therapy-ranks:


```{r echo=FALSE, eval=TRUE}
# Show the table.
therapy_ranks <- merger_ranks
names(therapy_ranks) = c('Triplet #', 'Therapy only', 'Combined', 'Drug only')
ketable(therapy_ranks,
        caption ="Ranked therapies within matched-patient triplets (1=best)
        {#tbl-therapy-ranks}")
```

These (@tbl-therapy-mean-ranks) are the average ranks for the three groups (1
= best, 3 = worst):

```{r echo=FALSE, eval=TRUE}
therapy_rank_means <- as.data.frame(colMeans(therapy_ranks[,-1]))
names(therapy_rank_means) <- c('Mean rank')
ketable(therapy_rank_means,
        caption ="Mean rank of therapy in matched triplets {#tbl-therapy-mean-ranks}")
```

In these hypothetical data, the average rank for the drug and therapy
regime is `r round(actual_mean_rank, 2)`.
Is it likely that the regimes do not "really" differ
with respect to effectiveness, and that the drug and therapy regime came
out with the best rank just by the luck of the draw? We test by asking,
"If there is no difference, what is the probability that the treatment
of interest will get an average rank this good, just by chance?"

We proceed exactly as with the solution for the merger problem (see
above).

In the above problems, we did not concern ourselves with chance outcomes
for the other therapies (or the matched firms) because they were not our
primary focus. If, in actual fact, one of them had done exceptionally
well or poorly, we would have paid little notice because their
performance was not the object of the study. We needed, therefore, only
to guard against the possibility that chance good luck for *our therapy
of interest* might have led us to a hasty conclusion.

Suppose now that we are not interested primarily in the combined
drug-therapy treatment, and that we have three treatments being tested,
all on equal footing. It is no longer sufficient to ask the question
"What is the probability that the combined therapy could come out this
well just by chance?" We must now ask "What is the probability that
*any* of the therapies could have come out this well by chance?"
(Perhaps you can guess that this probability will be higher than the
probability that our *chosen therapy* will do so well by chance.)

Here is a resampling procedure that will answer this question:

1.  Put the numbers "1", "2" and "3" (corresponding to ranks) in a bucket
2.  Shuffle the numbers and deal them out to three locations that
    correspond to treatments (call the locations "t1," "t2," and "t3")
3.  Repeat step two another 32 times (for a total of 33 repetitions, for
    33 matched triplets)
4.  Find the average rank for each location (treatment).
5.  Record the minimum (best) score.
6.  Repeat steps 2-4, say, 10,000 times.
7.  Find out how often the minimum average rank for any treatment is as
    low as `r round(actual_mean_rank, 2)`.

```{python}
import numpy as np
import matplotlib.pyplot as plt

# set up the random number generator
rnd = np.random.default_rng()

# Set the number of trials
n_iters = 10_000

# An empty array to store the trial results.
results = np.zeros(n_iters)

# Step 1 above.
bucket = np.array([1, 2, 3])

n_rows = 33

# Do 10000 trials (step 6).
for i in range(n_iters):
    # Prepare arrays to store shuffled row results.
    t1 = np.zeros(n_rows)
    t2 = np.zeros(n_rows)
    t3 = np.zeros(n_rows)
    # Step 3 (for all 33 rows).
    for row_no in range(n_rows):
        # Step 2.
        shuffled = rnd.permuted(bucket)
        # Step 2 continued.
        t1[row_no] = shuffled[0]
        t2[row_no] = shuffled[1]
        t3[row_no] = shuffled[2]
    # Step 4.
    m_t1 = np.mean(t1)
    m_t2 = np.mean(t2)
    m_t3 = np.mean(t3)
    # Step 5.
    means = np.array([m_t1, m_t2, m_t3])
    min_mean = np.min(means)
    results[i] = min_mean

plt.hist(results, bins=50)
plt.title('Distribution of minimum average rank')

# Step 7.
k = np.sum(results <= 1.48)
kk = k / n_iters

print('Proportion minimum average rank <= 1.48:', kk)
```

```{r}
# Set the number of trials
n_iters <- 10000

# An empty array to store the trial results.
results <- numeric(n_iters)

# Step 1 above.
bucket <- c(1, 2, 3)

n_rows <- 33

# Do 10000 trials.
for (i in 1:n_iters) {
    # Prepare vectors to store shuffled row results.
    t1 <- numeric(n_rows)
    t2 <- numeric(n_rows)
    t3 <- numeric(n_rows)
    # Step 3 (for all 33 rows).
    for (row_no in 1:n_rows) {
        # Step 2.
        shuffled <- sample(bucket)
        # Step 2 continued.
        t1[row_no] <- shuffled[1]
        t2[row_no] <- shuffled[2]
        t3[row_no] <- shuffled[3]
    }
    # Step 4.
    m_t1 <- mean(t1)
    m_t2 <- mean(t2)
    m_t3 <- mean(t3)
    # Step 5.
    means <- c(m_t1, m_t2, m_t3)
    min_mean <- min(means)
    results[i] <- min_mean
}

hist(results, breaks=50,
     main='Distribution of minimum average rank')

# Step 7.
k <- sum(results <= 1.49)
kk <- k / n_iters

message('Proportion minimum average rank <= 1.48: ', kk)
```

:::
<!---
End of notebook
-->

Interpretation: We did 10,000 random shufflings of 33 ranks, apportioned to
three "treatments".  Of these, about `r round(get_var('kk') * 100, 1)` percent
produced, for the best treatment in the three, an average as low as the
observed average, therefore we rule out chance as an explanation for the
success of the combined therapy.

An interesting feature of the mergers (or depression treatment) problem is that
it would be hard to find a conventional test that would handle this three-way
comparison in an efficient manner. Certainly it would be impossible to find
a test that does not require formulae and tables that only a talented
professional statistician could manage satisfactorily, and even they are not
likely to fully understand those formulaic procedures.

## Technical note

Some of the tests introduced in this chapter are similar to standard
non-parametric rank and sign tests. They differ less in the structure of the
test statistic than in the way in which significance is assessed (the
comparison is to multiple simulations of a model based on the benchmark
hypothesis, rather than to critical values calculated analytically).
