---
resampling_with:
    ed2_fname: 10-Chap-6
jupyter:
  jupytext:
    notebook_metadata_filter: all,-language_info
    split_at_heading: true
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.14.6
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

```{r setup, include=FALSE}
source("_common.R")
```

# Probability Theory, Part 2: Compound Probability {#sec-compound-probability}

## Introduction

In this chapter we will deal with what are usually called "probability
problems" rather than the "statistical inference problems" discussed in
later chapters. The difference is that for probability problems we begin
with a knowledge of the properties of the universe with which we are
working. (See @sec-what-is-resampling on the definition of resampling.)

We start with some basic problems in probability.  To make sure we do know the properties of the universe we are working with, we start with poker, and a pack of cards.  Working with some poker problems, we rediscover the fundamental distinction between sampling *with* and *without* replacement.

## Introducing a poker problem: one pair (two of a kind) {#sec-one-pair}

What is the chance that the first five cards chosen from a deck of 52
(bridge/poker) cards will contain two (and only two) cards of the same
denomination (two 3's for example)? (Please forgive the rather sterile
unrealistic problems in this and the other chapters on probability. They
reflect the literature in the field for 300 years. We'll get more
realistic in the statistics chapters.)

We shall estimate the odds the way that gamblers have estimated gambling odds
for thousands of years. First, check that the deck is a standard deck and is
not missing any cards. (Overlooking such small but crucial matters often leads
to errors in science.) Shuffle thoroughly until you are satisfied that the
cards are randomly distributed. (It is surprisingly hard to shuffle well.) Then
deal five cards, and mark down whether the hand does or does not contain a pair
of the same denomination.

At this point, we must decide whether three of a kind, four of a kind or two
pairs meet our criterion for a pair. Since our criterion is "two and only
two," we decide *not* to count them.

Then replace the five cards in the deck, shuffle, and deal again. Again
mark down whether the hand contains one pair of the same denomination.
Do this many times. Then count the number of hands with one pair, and
figure the proportion (as a percentage) of all hands.

```{python eval=TRUE, echo=FALSE}
n_deals = 25
```

@tbl-one-pair has the results of `r py$n_deals` hands of this procedure.

```{python eval=TRUE, echo=FALSE}
from itertools import product
suit_chars = ['\N{Black Spade Suit}',
    '\N{White Heart Suit}',
    '\N{White Diamond Suit}',
    '\N{Black Club Suit}']
denom = ['Ace'] + [str(s) for s in range(2, 11)] + ['Jack', 'Queen', 'King']
values = list(range(1, 14)) * 4
deck = [f'{d} {s}' for s, d in product(suit_chars, denom)]
deck_values = dict(zip(deck, values))
```

```{python eval=TRUE, echo=FALSE, results="asis", message=FALSE}
import numpy as np
import pandas as pd
seed = 1937
rnd = np.random.default_rng(seed)
df = pd.DataFrame(columns=['Card 1',
                           'Card 2',
                           'Card 3',
                           'Card 4',
                           'Card 5',
                           'One pair?'],
                  index=pd.Index(range(1, n_deals + 1), name='Hand'))
for i in range(1, n_deals + 1):
    cards = rnd.choice(deck, size=5, replace=False)
    df.loc[i, 'Card 1':'Card 5'] = cards
    values = [deck_values[c] for c in cards]
    counts = np.bincount(values)
    df.loc[i, 'One pair?'] = 'Yes' if np.sum(counts == 2) == 1 else 'No'

def add_pct(df):
    pct = int(np.round(np.sum(df['One pair?'] == 'Yes') / len(df) * 100))
    n_cols = len(df.columns)
    df.loc['', :] = [''] * n_cols
    df.loc['**% Yes**', :] = [''] * (n_cols - 1) + [f'{pct}%']
    return pct

pct = add_pct(df)
cards_tab = df.to_markdown(tablefmt="grid")
print(f'{cards_tab}\n\n: Results of {n_deals} hands '
      'for the problem "one pair" {#tbl-one-pair}')
```

In this series of `r py$n_deals` experiments, `r py$pct` percent of the hands
contained one pair, and therefore `r py$pct / 100` is our estimate (for the
time being) of the probability that one pair will turn up in a poker hand. But
we must notice that this estimate is based on only `r py$n_deals` hands, and
therefore might well be fairly far off the mark (as we shall soon see).

This experimental "resampling" estimation does not require a deck of cards. For
example, one might create a 52-sided die, one side for each card in the deck,
and roll it five times to get a "hand." But note one important part of the
procedure: No single "card" is allowed to come up twice in the same set of five
spins, just as no single card can turn up twice or more in the same hand. If
the same "card" did turn up twice or more in a dice experiment, one could
pretend that the roll had never taken place; this procedure is necessary to
make the dice experiment analogous to the actual card-dealing situation under
investigation. Otherwise, the results will be slightly in error. This type of
sampling is "sampling without replacement," because each card is *not replaced*
in the deck prior to dealing the next card (that is, prior to the end of the
hand).

## A first approach to the one-pair problem with code

We could also approach this problem using random numbers from the computer to
simulate the values.

Let us first make some numbers from which to sample.  We want to simulate a
deck of playing cards analogous to the real cards we used previously. We don't
need to simulate all the features of a deck, but only the features that matter
for the problem at hand.  In our case, the feature that matters is the face
value.  We require a deck with four "1"s, four "2"s, etc., up to four "13"s,
where 1 is an Ace, and 13 is a King. The suits don't matter for our present
purposes.

We first first make {{< var an_array >}} to represent the face values in one
suit.

```{python opts.label="py_ed"}
# Card values 1 through 13 (1 up to, not including 14).
one_suit = np.arange(1, 14)
one_suit
```

```{r opts.label="r_ed"}
one_suit <- 1:13
one_suit
```

We have the face values for one suit, but we need the face values for whole
deck of cards — four suits.  We do this by making a new {{< var array >}} that
consists of four repeats of `one_suit`:

```{python opts.label="py_ed"}
# Repeat the one_suit array four times
deck = np.repeat(one_suit, 4)
deck
```

```{r opts.label="r_ed"}
# Repeat the one_suit vector four times
deck <- rep(one_suit, 4)
deck
```

## Shuffling the deck with {{< var lang >}} {#sec-shuffling-deck}

At this point we have a complete deck in the variable `deck` . But that "deck"
is
[in the same order as a new deck of cards]{.r}
[ordered by value, first ones (Aces) then 2s and so on]{.python}.
If we do not shuffle the deck, the results will be predictable.  Therefore, we
would like to select five of these "cards" (52 values) at random.  There are
two ways of doing this.  The first is to use the
[`sample`]{.r}'rnd.choice`]{.python} tool in the familiar way, to choose 5
values at random from this strictly ordered deck.  We want to draw these cards
*without replacement* (of which more later).  *Without replacement* means that
once we have drawn a particular value, we cannot draw that value a second time
— just as you cannot get the same card twice in a hand when the dealer deals
you a hand of five cards.

::: python
So far, each of our uses of `rnd.choice` has done sampling *with replacement*,
where you *can* get the same item more than once in a particular sample.  Here we need *without replacement*.  `rnd.choice` has an argument you can send, called `replace`, to tell it whether to replace values when drawing the sample.  We have not used that argument so far, because the default is `True` — sampling *with replacement*.  Here we need to use the argument — `replace=False` — to get sampling *without replacement*.
:::

```{python opts.label="py_ed"}
# One hand, sampling from the deck without replacement.
hand = rnd.choice(deck, size=5, replace=False)
hand
```

::: r
As you saw in @sec-shuffling, the *default* behavior of `sample` is to sample
*without replacement*, so simply omit the `replace=TRUE` argument to `sample` to get sampling without replacement:
:::

```{r opts.label="r_ed"}
# One hand, sampling from the deck without replacement.
hand <- sample(deck, size=5)
hand
```

The above is one way to get a random hand of five cards from the deck.  Another
way is to use [`sample`]{.r}[the `rnd.permuted` function]{.python} to shuffle
the whole `deck` of 52 "cards" into a random order, just as a dealer would
shuffle the deck before dealing.  Then we could take — for example — the first
five cards from the shuffled deck to give a random hand.  See @sec-shuffling
for more on [this use of `sample`]{.r}[`rnd.permuted`]{.python}.

```{python opts.label="py_ed"}
# Shuffle the whole 52 card deck.
shuffled = rnd.permuted(deck)
# The "cards" are now in random order.
shuffled
```

```{r opts.label="r_ed"}
# Shuffle the whole 52 card deck.
shuffled <- sample(deck)
# The "cards" are now in random order.
shuffled
```

Now we can get our `hand` by taking the first five cards from the `deck`:

```{python opts.label="py_ed"}
# Select the first five "cards" from the shuffled deck.
hand = shuffled[:5]
hand
```

```{r opts.label="r_ed"}
# Select the first five "cards" from the shuffled deck.
hand <- shuffled[1:5]
hand
```

You have seen that we can use one of two procedures to a get random sample of
five cards from `deck`, drawn without replacement:

1. Using [`sample` with `size=5`]{.r}[`rnd.choice` with `size=5` and
   `replace=False`]{.python}
   to take the random sample directly from `deck`, or
2. shuffling the entire `deck` and then taking the first five "cards" from the
   result of the shuffle.

Either is a valid way of getting five cards at random from the `deck`.  It's up
to us which to choose — we slightly prefer to shuffle and take the first five,
because it is more like the physical procedure of shuffling the deck and
dealing, but which you prefer, is up to you.

### A first-pass computer solution to the one-pair problem

Choosing the shuffle deal way, the {{< var cell >}} to generate one hand is:

```{python opts.label="py_ed"}
shuffled = rnd.permuted(deck)
hand = shuffled[:5]
hand
```

```{r opts.label="r_ed"}
shuffled <- sample(deck)
hand <- shuffled[1:5]
hand
```

Without doing anything further, we could run this {{< var cell >}} many times,
and each time, we could note down whether the particular `hand` had exactly one pair or not.

@tbl-one-pair-numbers has the result of running that procedure `r py$n_deals`
times:

```{python eval=TRUE, echo=FALSE, results="asis", message=FALSE}
df = pd.DataFrame(columns=['Card 1',
                           'Card 2',
                           'Card 3',
                           'Card 4',
                           'Card 5',
                           'One pair?'],
                  index=pd.Index(range(1, n_deals + 1), name='Hand'))
deck = np.repeat(np.arange(1, 14), 4)
for i in range(1, n_deals + 1):
    cards = rnd.choice(deck, size=5, replace=False)
    df.loc[i, 'Card 1':'Card 5'] = cards
    counts = np.bincount(cards)
    df.loc[i, 'One pair?'] = 'Yes' if np.sum(counts == 2) == 1 else 'No'

pct = add_pct(df)
cards_tab = df.to_markdown(tablefmt="grid")
print(f'{cards_tab}\n\n: Results of {n_deals} hands '
      'using random numbers {#tbl-one-pair-numbers}')
```

## Finding exactly one pair using code

Thus far we have had to look ourselves at the set of cards, or at the numbers,
and decide if there was exactly one pair.  We would like the computer to do
this for us.   Let us stay with the numbers we generated above by dealing the
random `hand` from the `deck` of numbers.  To find pairs, we will go through the following procedure:

* For each possible value (1 through 13), count the number of times each value
  has occurred in `hand`.  Call the result of this calculation — `repeat_nos`.
* Select `repeat_nos` values equal to 2;
* Count the number of "2" values in `repeat_nos`.  This the number of pairs,
  and excludes three of a kind or four a kind.
* If the number of pairs is exactly one, label the `hand` as "Yes", otherwise
  label it as "No".

## Finding number of repeats using {{< var bincount >}}

Consider the following 5-card "hand" of values:

```{python, opts.label="py_ed"}
hand = np.array([5, 7, 5, 4, 7])
```

```{r, opts.label="r_ed"}
hand <- c(5, 7, 5, 4, 7)
```

This hand represents a pair of 5s and a pair of 7s.

Want to detect the number of repeats for each possible card value, 1 through
13.  Let's say we are looking for 5s.   We can detect which of the values are equal to 5 by making a Boolean array, where there is True for a value equal to 5, and False otherwise:

```{python, opts.label="py_ed"}
is_5 = hand == 5
is_5
```

```{r, opts.label="r_ed"}
is_5 <- hand == 5
```

We can then count the number of 5s with:

```{python, opts.label="py_ed"}
np.sum(is_5)
```

```{r, opts.label="r_ed"}
sum(is_5)
```

In one {{< var cell >}}:

```{python, opts.label="py_ed"}
number_of_5s = np.sum(hand == 5)
number_of_5s
```

```{r, opts.label="r_ed"}
number_of_5s <- sum(hand == 5)
number_of_5s
```

We could do this laborious task for every possible card value (1 through 13):

```{python, opts.label="py_ed"}
number_of_1s = np.sum(hand == 1)  # Number of aces in hand
number_of_2s = np.sum(hand == 2)  # Number of 2s in hand
number_of_3s = np.sum(hand == 3)
number_of_4s = np.sum(hand == 4)
number_of_5s = np.sum(hand == 5)
number_of_6s = np.sum(hand == 6)
number_of_7s = np.sum(hand == 7)
number_of_8s = np.sum(hand == 8)
number_of_9s = np.sum(hand == 9)
number_of_10s = np.sum(hand == 10)
number_of_11s = np.sum(hand == 11)
number_of_12s = np.sum(hand == 12)
number_of_13s = np.sum(hand == 13)  # Number of Kings in hand.
```

```{r, opts.label="r_ed"}
number_of_1s <- sum(hand == 1)  # Number of aces in hand
number_of_2s <- sum(hand == 2)  # Number of 2s in hand
number_of_3s <- sum(hand == 3)
number_of_4s <- sum(hand == 4)
number_of_5s <- sum(hand == 5)
number_of_6s <- sum(hand == 6)
number_of_7s <- sum(hand == 7)
number_of_8s <- sum(hand == 8)
number_of_9s <- sum(hand == 9)
number_of_10s <- sum(hand == 10)
number_of_11s <- sum(hand == 11)
number_of_12s <- sum(hand == 12)
number_of_13s <- sum(hand == 13)  # Number of Kings in hand.
```

Above, we store the result for each card in a separate variable; this is
inconvenient, because we would have to go through each variable checking for a
pair (a value of 2).  It would be more convenient to store these results in {{< var an_array >}}.  One way to do that would be to store the result for card value 1 at position (index[, offset]{.python}) 1, the result for value 2 at position 2, and so on, like this:

```{python, opts.label="py_ed"}
# Make array length 14.  We don't use position (offset) 0, and the last
# position (offset) in this array will be 13.
repeat_nos = np.zeros(14)
repeat_nos[1] = np.sum(hand == 1)  # Number of aces in hand
repeat_nos[2] = np.sum(hand == 2)  # Number of 2s in hand
repeat_nos[3] = np.sum(hand == 3)
repeat_nos[4] = np.sum(hand == 4)
repeat_nos[5] = np.sum(hand == 5)
repeat_nos[6] = np.sum(hand == 6)
repeat_nos[7] = np.sum(hand == 7)
repeat_nos[8] = np.sum(hand == 8)
repeat_nos[9] = np.sum(hand == 9)
repeat_nos[10] = np.sum(hand == 10)
repeat_nos[11] = np.sum(hand == 11)
repeat_nos[12] = np.sum(hand == 12)
repeat_nos[13] = np.sum(hand == 13)  # Number of Kings in hand.
# Show the result
repeat_nos
```

```{r, opts.label="r_ed"}
# Make vector length 13, with one element for each card value.
repeat_nos <- numeric(13)
repeat_nos[1] <- sum(hand == 1)  # Number of aces in hand
repeat_nos[2] <- sum(hand == 2)  # Number of 2s in hand
repeat_nos[3] <- sum(hand == 3)
repeat_nos[4] <- sum(hand == 4)
repeat_nos[5] <- sum(hand == 5)
repeat_nos[6] <- sum(hand == 6)
repeat_nos[7] <- sum(hand == 7)
repeat_nos[8] <- sum(hand == 8)
repeat_nos[9] <- sum(hand == 9)
repeat_nos[10] <- sum(hand == 10)
repeat_nos[11] <- sum(hand == 11)
repeat_nos[12] <- sum(hand == 12)
repeat_nos[13] <- sum(hand == 13)  # Number of Kings in hand.
# Show the result
repeat_nos
```

You may recognize all this repetitive typing as a good sign we could use a `for` loop to do the work — er — for us.

```{python, opts.label="py_ed"}
repeat_nos = np.zeros(14)
for i in range(14):  # Set i to be first 0, then 1, ... through 13.
    repeat_nos[i] = np.sum(hand == i)
# Show the result
repeat_nos
```

::: python
Notice that we started our loop by checking for values equal to 0, and then
values equal to 1 and so on.  By our definition of the deck, no card can have
value 0, so the first time through this loop, we will always get a count of 0.  We could saved ourselves a tiny amount of computing time by missing out that step by using `for i in range(1, 14):` instead, but it's a little bit neater to leave in the default start at 0, at a tiny cost in wasted computer effort, in this case.
:::

```{r, opts.label="r_ed"}
repeat_nos <- numeric(13)
for (i in 1:13) {  # Set i to be first 1, then 2, ... through 13.
    repeat_nos[i] <- sum(hand == i)
}
# Show the result
repeat_nos
```

In our particular `hand`, after we have done the count for 7s, we will always
get 0 for card values 8, 9 ... 13, because 7 was the highest card (maximum
value) for our particular `hand`.  As you might expect, there is a [an
R]{.r}[a Numpy]{.python} function [`max`]{.r}[`np.max`]{.python} that will
quickly tell us the maximum value in the hand:

```{python, opts.label="py_ed"}
np.max(hand)
```

```{r, opts.label="r_ed"}
max(hand)
```

We can use [`max`]{.r}[`np.max`]{.python} to make our loop more efficient, by
stopping our checks when we've reached the maximum value, like this:

```{python, opts.label="py_ed"}
max_value = np.max(hand)
# Only make an array large enough to house counts for the max value.
repeat_nos = np.zeros(max_value + 1)
for i in range(max_value + 1):  # Set i to 0, then 1 ... through max_value
    repeat_nos[i] = np.sum(hand == i)
# Show the result
repeat_nos
```

```{r, opts.label="r_ed"}
max_value <- max(hand)
# Only make a vector large enough to house counts for the max value.
repeat_nos <- numeric(max_value)
for (i in 1:max_value) {  # Set i to 0, then 1 ... through max_value
    repeat_nos[i] <- sum(hand == i)
}
# Show the result
repeat_nos
```

In fact, this is exactly what the function
[`tabulate`]{.r}[`np.bincount`]{.python} does, so we can use that function instead of our loop, to do the same job:

```{python, opts.label="py_ed"}
repeat_nos = np.bincount(hand)
repeat_nos
```

```{r, opts.label="r_ed"}
repeat_nos <- tabulate(hand)
repeat_nos
```

## Looking for hands with exactly one pair

Now we have `repeat_nos`, we can proceed with the rest of the steps above.

We can count the number of cards that have exactly two repeats:

```{python, opts.label="py_ed"}
repeat_nos == 2
```

```{python, opts.label="py_ed"}
n_pairs = np.sum(repeat_nos == 2)
# Show the result
n_pairs
```

```{r, opts.label="r_ed"}
repeat_nos == 2
```

```{r, opts.label="r_ed"}
n_pairs <- sum(repeat_nos == 2)
# Show the result
n_pairs
```

The hand is of interest to us only if the number of pairs is exactly 1:

```{python, opts.label="py_ed"}
n_pairs == 1
```

```{python, opts.label="py_ed"}
# Check whether there is exactly one pair in this hand.
np.sum(n_pairs == 1)
```

We now have all the machinery to use {{< var lang >}} for all the logic in simulating multiple hands, and checking for exactly one pair.

Now let's use {{< var lang >}} to do the full job of dealing many hands and
finding pairs in each one.  To do this, we repeat the procedure above using
a `for` loop. This commands the program to do ten thousand repeats of the
statements in the "loop" [between the start `{` and end `}` curly
braces]{.r}[(indented statements)]{.python}.

In the *body* of the loop (the part that gets repeated for each trial) we:

* Shuffle the `deck`.
* Deal ourselves a new `hand`.
* Calculate the `repeat_nos` *for this new hand*.
* Calculate the number of pairs from `repeat_nos`; store this as `n_pairs`.
* Put `n_pairs` for this repetition into the correct place in the scoring {{<
  var array >}} `z`.

With that we end a single trial, and go back to the beginning, until we have
done this 10000 times.

When those 10000 repetitions are over, the computer moves on to count (`sum`)
the number of "1's" in the score-keeping {{< var array >}} `z`, each "1"
indicating a hand with a pair.  We store this count at location `k`. We divide
`k` by 10000 to get the *proportion* of hands that had one pair, and we
[`message`]{.r}[`print`]{.python}
the result of `k` to the screen.

::: {.notebook name="one_pair" title="One pair"}

::: nb-only
This is a simulation to find the probability of exactly one pair in a poker
hand of five cards.
:::

```{python opts.label="py_ed"}
import numpy as np
rnd = np.random.default_rng()
```

```{python opts.label="py_ed"}
# Create a bucket (vector) called a with four "1's," four "2's," four "3's,"
# etc., to represent a deck of cards
one_suit = np.arange(1, 14)
one_suit
```

```{python opts.label="py_ed"}
# Repeat values for one suit four times to make a 52 card deck of values.
deck = np.repeat(one_suit, 4)
deck
```

```{python opts.label="py_ed"}
# Array to store result of each trial.
z = np.zeros(10000)

# Repeat the following steps 10000 times
for i in range(10000):
    # Shuffle the deck
    shuffled = rnd.permuted(deck)

    # Take the first five cards to make a hand.
    hand = shuffled[:5]

    # How many pairs?
    # Counts for each card rank.
    repeat_nos = np.bincount(hand)
    n_pairs = np.sum(repeat_nos == 2)

    # Keep score of # of pairs
    z[i] = n_pairs

    # End loop, go back and repeat

# How often was there 1 pair?
k = np.sum(z == 1)

# Convert to proportion.
kk = k / 10000

# Show the result.
print(kk)
```

```{r opts.label="r_ed"}
# Create a bucket (vector) called a with four "1's," four "2's," four "3's,"
# etc., to represent a deck of cards
one_suit = 1:13
one_suit
```

```{r opts.label="r_ed"}
# Repeat values for one suit four times to make a 52 card deck of values.
deck <- rep(one_suit, 4)
deck
```

```{r opts.label="r_ed"}
# Vector to store result of each trial.
z <- numeric(10000)

# Repeat the following steps 10000 times
for (i in 1:10000) {
    # Shuffle the deck
    shuffled <- sample(deck)

    # Take the first five cards to make a hand.
    hand = shuffled[1:5]

    # How many pairs?
    # Counts for each card rank.
    repeat_nos <- tabulate(hand)
    n_pairs <- sum(repeat_nos == 2)

    # Keep score of # of pairs
    z[i] <- n_pairs

    # End loop, go back and repeat
}

# How often was there 1 pair?
k <- sum(z == 1)

# Convert to proportion.
kk = k / 10000

# Show the result.
message(kk)
```

:::

```{r eval=TRUE, echo=FALSE}
rkk <- round(get_var('kk'), 3)
```

In one run of the program, the result in `kk` was `r rkk`, so our estimate
would be that the probability of a single pair is `r rkk`.

How accurate are these resampling estimates? The accuracy depends on the
*number of hands* we deal — the more hands, the greater the accuracy. If
we were to examine millions of hands, 42 percent would contain a pair
each; that is, the chance of getting a pair in the long run is 42
percent. It turns out the estimate of `r py$pct` percent based on 
`r py$n_deals` hands in @tbl-one-pair is fairly close to the long-run
estimate, though whether or not it is close *enough* depends on one's needs of
course. If you need great accuracy, deal many more hands.

A note on the `deck`s, `hand`s, `repeat_nos`s in the above program, etc.:
These "variables" are called "{{< var array >}}"s in {{< var lang >}}.
[A *vector*]{.r}[An *array*]{.python} is an array (sequence) of elements that
gets filled with numbers as {{< var lang >}} conducts its operations.

To help keep things straight (though the program does not require it), we
often use `z` to name the {{< var array >}} that collects all the trial
results, and `k` to denote our overall summary results. Or you could call it
something like `scoreboard` — it's up to you.

How many trials (hands) should be made for the estimate? There is no
easy answer.[^how-many-trials] One useful device is to run several (perhaps
ten) equal sized sets of trials, and then examine whether the proportion of
pairs found in the entire group of trials is very different from the
proportions found in the various subgroup sets. If the proportions of pairs in
the various subgroups differ greatly from one another or from the overall
proportion, then keep running additional larger subgroups of trials until the
variation from one subgroup to another is sufficiently small for your purposes.
While such a procedure would be impractical using a deck of cards or any other
physical means, it requires little effort with the computer and {{< var lang
>}}.

[^how-many-trials]: One simple rule-of-thumb is to quadruple the original
    number. The reason for quadrupling is that four times as many iterations
    (trials) of this resampling procedure give *twice* as much accuracy (as
    measured by the standard deviation, the most frequent measurement of
    accuracy). That is, the error decreases with the square root of the number
    of iterations. If you see that you need *much* more accuracy, then
    *immediately* increase the number of iterations even more than four times
    — perhaps ten or a hundred times.

## Two more tntroductory poker problems

Which is more likely, a poker hand with two pairs, or a hand with three
of a kind? This is a *comparison* problem, rather than a problem in
*absolute* estimation as was the previous example.

In a series of 100 "hands" that were "dealt" using random numbers, four
hands contained two pairs, and two hands contained three of a kind. Is
it safe to say, on the basis of these 100 hands, that hands with two
pairs are more frequent than hands with three of a kind? To check, we
deal another 300 hands. Among them we see fifteen hands with two pairs
(3.75 percent) and eight hands with three of a kind (2 percent), for a
total of nineteen to ten. Although the difference is not enormous, it is
reasonably clear-cut. Another 400 hands might be advisable, but we shall
not bother.

Earlier I obtained forty-four hands with *one* pair each out of 100
hands, which makes it quite plain that *one* pair is more frequent than
*either* two pairs or three-of-a-kind. Obviously, we need *more* hands
to compare the odds in favor of two pairs with the odds in favor of
three-of-a-kind than to compare those for one pair with those for either
two pairs or three-of-a-kind. Why? Because the difference in odds
between one pair, and either two pairs or three-of-a-kind, is much
greater than the difference in odds between two pairs and
three-of-a-kind. This observation leads to a general rule: The closer
the odds between two events, the *more trials* are needed to determine
which has the higher odds.

Again it is interesting to compare the odds with the formulaic
mathematical computations, which are 1 in 21 (4.75 percent) for a hand
containing two pairs and 1 in 47 (2.1 percent) for a hand containing
three-of-a-kind — not too far from the estimates of .0375 and .02
derived from simulation.

To handle the problem with the aid of the computer, we simply need to
estimate the proportion of hands having triplicates and the proportion
of hands with two pairs, and compare those estimates.

To estimate the hands with three-of-a-kind, we can use a notebook just
like "One Pair" earlier, except using `repeat_nos == 3` to search for
triplicates instead of duplicates. The program, then, is:

::: {.notebook name="three_of_a_kind" title="Three of a kind"}

::: nb-only
We count the number of times we get three of a kind in a random hand of five
cards.
:::

```{python, opts.label="py_ed"}
import numpy as np
rnd = np.random.default_rng()
```

```{python opts.label="py_ed"}
# Create a bucket (vector) called a with four "1's," four "2's," four "3's,"
# etc., to represent a deck of cards
one_suit = np.arange(1, 14)
# Repeat values for one suit four times to make a 52 card deck of values.
deck = np.repeat(one_suit, 4)
```

```{python opts.label="py_ed"}
triples_per_trial = np.zeros(10000)

# Repeat the following steps 10000 times
for i in range(10000):
    # Shuffle the deck
    shuffled = rnd.permuted(deck)

    # Take the first five cards.
    hand = shuffled[:5]

    # How many triples?
    repeat_nos = np.bincount(hand)
    n_triples = np.sum(repeat_nos == 3)

    # Keep score of # of triples
    triples_per_trial[i] = n_triples

    # End loop, go back and repeat

# How often was there 1 pair?
n_triples = np.sum(triples_per_trial == 1)

# Convert to proportion
print(n_triples / 10000)
```

```{r, opts.label="r_ed"}
one_suit <- 1:13
deck <- rep(one_suit, 4)
```

```{r opts.label="r_ed"}
triples_per_trial <- numeric(10000)

# Repeat the following steps 10000 times
for (i in 1:10000) {
    # Shuffle the deck
    shuffled <- sample(deck)

    # Take the first five cards.
    hand <- shuffled[1:5]

    # How many triples?
    repeat_nos <- tabulate(hand)
    n_triples <- sum(repeat_nos == 3)

    # Keep score of # of triples
    triples_per_trial[i] <- n_triples

    # End loop, go back and repeat
}

# How often was there 1 pair?
n_triples <- sum(triples_per_trial == 1)

# Convert to proportion
message(n_triples / 10000)
```

:::

To estimate the probability of getting a two-pair hand, we revert to the
original program (counting pairs), except that we examine all the
results in the score-keeping vector `z` for hands in which we had *two* pairs,
instead of *one* .

::: {.notebook name="two_pairs" title="Two pairs"}

::: nb-only
We count the number of times we get two pairs in a random hand of five cards.
:::

```{python, opts.label="py_ed"}
import numpy as np
rnd = np.random.default_rng()

one_suit = np.arange(1, 14)
deck = np.repeat(one_suit, 4)
```

```{python opts.label="py_ed"}
pairs_per_trial = np.zeros(10000)

# Repeat the following steps 10000 times
for i in range(10000):
    # Shuffle the deck
    shuffled = rnd.permuted(deck)

    # Take the first five cards.
    hand = shuffled[:5]

    # How many pairs?
    # Counts for each card rank.
    repeat_nos = np.bincount(hand)
    n_pairs = np.sum(repeat_nos == 2)

    # Keep score of # of pairs
    pairs_per_trial[i] = n_pairs

    # End loop, go back and repeat

# How often were there 2 pairs?
n_two_pairs = np.sum(pairs_per_trial == 2)

# Convert to proportion
print(n_two_pairs / 10000)
```

```{r, opts.label="r_ed"}
deck <- rep(1:13, 4)
```

```{r opts.label="r_ed"}
pairs_per_trial <- numeric(10000)

# Repeat the following steps 10000 times
for (i in 1:10000) {
    # Shuffle the deck
    shuffled <- sample(deck)

    # Take the first five cards.
    hand <- shuffled[1:5]

    # How many pairs?
    # Counts for each card rank.
    repeat_nos <- tabulate(hand)
    n_pairs <- sum(repeat_nos == 2)

    # Keep score of # of pairs
    pairs_per_trial[i] <- n_pairs

    # End loop, go back and repeat
}

# How often were there 2 pairs?
n_two_pairs <- sum(pairs_per_trial == 2)

# Convert to proportion
print(n_two_pairs / 10000)
```

:::

For efficiency (though efficiency really is not important here because
the computer performs its operations so cheaply) we could develop both
estimates in a single program by simply generating 10000 hands, and count
the number with three-of-a-kind and the number with two pairs.

Before we leave the poker problems, we note a difficulty with Monte
Carlo simulation. The probability of a royal flush is so low (about one
in half a million) that it would take much computer time to compute. On
the other hand, considerable inaccuracy is of little matter. Should one
care whether the probability of a royal flush is 1/100,000 or 1/500,000?

## The concepts of replacement and non-replacement

In the poker example above, we *did not replace* the first card we drew.
If we were to replace the card, it would leave the probability the same
before the second pick as before the first pick. That is, the
conditional probability remains the same. *If we replace, conditions do
not change.* But if we do not replace the item drawn, the probability
changes from one moment to the next. (Perhaps refresh your mind with the
examples in the discussion of conditional probability including
@sec-example-four-events)

If we sample with replacement, the sample drawings remain *independent* of each
other — a topic addressed in @sec-independence.

In many cases, a key decision in modeling the situation in which we are
interested is whether to sample with or without replacement. The choice
must depend on the characteristics of the situation.

There is a close connection between the lack of finiteness of the
concept of universe in a given situation, and sampling with replacement.
That is, when the universe (population) we have in mind is not small, or
has no conceptual bounds at all, then the probability of each successive
observation remains the same, and this is modeled by sampling with
replacement. ("Not finite" is a less expansive term than "infinite,"
though one might regard them as synonymous.)

@sec-infinite-universes discusses problems whose appropriate concept of a universe is
finite, whereas @sec-finite-universes discusses problems whose appropriate concept
of a universe is not finite. This general procedure will be discussed
several times, with examples included.
