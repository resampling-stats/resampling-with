---
jupyter:
  jupytext:
    metadata_filter:
      notebook:
        additional: all
        excluded:
        - language_info
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.0'
      jupytext_version: 0.8.6
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
resampling_with:
    ed2_fname: 30-Exercise-sol
---

```{r setup, include=FALSE}
source("_common.R")
```

# Exercise Solutions {#sec-exercise-solutions}

:::{.callout-warning}
## Draft page partially ported from original PDF

This page is an automated and partial import from the [original second-edition
PDF](https://resample.com/content/text/30-Exercise-sol.pdf).

We are in the process of updating this page for formatting, and porting any
code from the original [RESAMPLING-STATS
language](http://www.statistics101.net) to Python and R.

Feel free to read this version for the sense, but expect there to be multiple
issues with formatting.

We will remove this warning when the page has adequate formatting, and we have
ported the code.
:::

## Solution to [paired differences exercise @sec-exr-paired-differences] {#sec-soln-paired-differences}

We suggested that you ignored the pairing of the before and after samples, and that is what we will do here.  Then we will extend the treatment to take the pairing into account.

::: {.notebook name="paired_differences_solution" title="Paired differences solution"}

```{python}
import numpy as np
import pandas as pd

rnd = np.random.default_rng()

df = pd.read_csv('data/hamilton.csv')
before = np.array(df['score_before'])
after = np.array(df['score_after'])

observed_diff = np.mean(after) - np.mean(before)

# Let us start with a permutation test.
both = np.concatenate([before, after])
n_before = len(before)

# Samples in the null world.
n_trials = 10_000
results = np.zeros(n_trials)
for i in range(n_trials):
    shuffled = rnd.permuted(both)
    fake_before = shuffled[:n_before]
    fake_after = shuffled[n_before:]
    fake_diff = np.mean(fake_after) - np.mean(fake_before)
    results[i] = fake_diff

# We are interested in fake differences that are larger
# in magnitude than the observed difference (hence "abs").
# Here we have no prior hypothesis about which direction the difference
# will go.
k = np.sum(np.abs(results) >= np.abs(observed_diff))
kk = k / n_trials
print('Permutation p null-world abs >= abs observed:', kk)

# Next a bootstrap test.
n_after = len(after)  # Of course, in our case, this will be == n_before
results = np.zeros(n_trials)
for i in range(n_trials):
    fake_before = rnd.choice(both, size=n_before)
    fake_after = rnd.choice(both, size=n_after)
    fake_diff = np.mean(fake_after) - np.mean(fake_before)
    results[i] = fake_diff

k = np.sum(np.abs(results) >= np.abs(observed_diff))
kk = k / n_trials
print('Bootstrap p null-world abs >= abs observed:', kk)
```

```{r}
df <- read.csv('data/hamilton.csv')
before <- df$score_before
after <- df$score_after

observed_diff <- mean(after) - mean(before)

# Let us start with a permutation test.
both <- c(before, after)
n_before <- length(before)

# Samples in the null world.
n_trials <- 10000
results <- numeric(n_trials)
for (i in 1:n_trials) {
    shuffled <- sample(both)
    fake_before <- shuffled[1:n_before]
    fake_after <- shuffled[(n_before + 1):length(both)]
    fake_diff <- mean(fake_after) - mean(fake_before)
    results[i] <- fake_diff
}

# We are interested in fake differences that are larger
# in magnitude than the observed difference (hence "abs").
# Here we have no prior hypothesis about which direction the difference
# will go.
k <- sum(abs(results) >= abs(observed_diff))
kk <- k / n_trials
message('Permutation p null-world abs >= abs observed: ', kk)

# Next a bootstrap test.
n_after <- length(after)  # Of course, in our case, this will be == n_before
results <- numeric(n_trials)
for (i in 1:n_trials) {
    fake_before <- sample(both, size=n_before, replace=TRUE)
    fake_after <- sample(both, size=n_after, replace=TRUE)
    fake_diff <- mean(fake_after) - mean(fake_before)
    results[i] <- fake_diff
}

k <- sum(abs(results) >= abs(observed_diff))
kk <- k / n_trials
message('Bootstrap p null-world abs >= abs observed: ', kk)
```

Finally we consider the pairs.  Here we *do* take the pairs into account.  We
have some reason to think that the patients or cars vary in some substantial way in their level of depression, or their tendency to break down, but we believe that the patients' *response* to treatment or the difference between the mechanics is the value of interest.

In that case, we are interested in the *differences* between the pairs.  In the
null world, these before / after (mechanic A / mechanic B) differences are
randomly â€” if there is no difference, then we can flip the before / after (A / B) pairs and be in the same world.

Notice though the flipping the before / after or A / B in the pair just changes the sign of the difference.

So we will simulate that effect, of flipping the values in the pair, by choosing a random sign for the pair, where -1 means pair is flipped, and 1 means pair is in original order.   We recalculated the mean difference with these random signs (flips) applied, and these will be our values in the null-world.

```{python}
# A test of paired difference with sign flips for the null world.
differences = after - before
observed_mdiff = np.mean(differences)
n_both = len(differences)

results = np.zeros(n_trials)
for i in range(n_trials):
    # Choose random signs to perform random flips of the pairs.
    signs = rnd.choice([-1, 1], size=n_both)
    # Do flips.
    fake_differences = signs * differences
    # Calculate mean difference and store result.
    results[i] = np.mean(fake_differences)

k = np.sum(np.abs(results) >= np.abs(observed_mdiff))
kk = k / n_trials
print('Sign-flip p null-world abs >= abs observed:', kk)
```

```{r}
# A test of paired difference with sign flips for the null world.
differences <- after - before
observed_mdiff <- mean(differences)
n_both <- length(differences)

results <- numeric(n_trials)
for (i in 1:n_trials) {
    # Choose random signs to perform random flips of the pairs.
    signs <- sample(c(-1, 1), size=n_both, replace=TRUE)
    # Do flips.
    fake_differences <- signs * differences
    # Calculate mean difference and store result.
    results[i] <- mean(fake_differences)
}

k <- sum(abs(results) >= abs(observed_mdiff))
kk <- k / n_trials
message('Sign-flip p null-world abs >= abs observed: ', kk)
```

Notice that the sign-flip test, in which we preserve the information about the patients / cars, is much more convincing than the permutation or bootstrap tests above, where we choose to ignore that information.

This can occur when the values within the pairs (rows) are similar to each
other, but less similar across different pairs (rows).

:::
<!---
End of notebook.
-->

## Solution to [seatbelt proportions exercise @sec-exr-seatbelt-proportions] {#sec-soln-seatbelt-proportions}

::: {.notebook name="seatbelt_proportion_solution" title="Seatbelt proportion solution"}

```{python}
import numpy as np
import matplotlib.pyplot as plt

rnd = np.random.default_rng()

pittsburgh = np.repeat(['seatbelt', 'none'], [36, 36])
n_pitts = len(pittsburgh)
chicago = np.repeat(['seatbelt', 'none'], [77, 52])
n_chicago = len(chicago)

n_trials = 10_000
results = np.zeros(n_trials)

for i in range(n_trials):
    fake_pitts = rnd.choice(pittsburgh, size=n_pitts)
    fake_chicago = rnd.choice(chicago, size=n_chicago)
    fake_p_pitts = np.sum(fake_pitts == 'seatbelt') / n_pitts
    fake_p_chicago = np.sum(fake_chicago == 'seatbelt') / n_chicago
    fake_p_diff = fake_p_pitts - fake_p_chicago
    results[i] = fake_p_diff

plt.hist(results, bins=25)
plt.title('Bootstrap distribution of p differences')
plt.xlabel('Bootstrap p differences')

p_limits = np.quantile(results, [0.025, 0.975])

print('95% percent limits for p differences:', p_limits)
```

```{r}
pittsburgh <- rep(c('seatbelt', 'none'), c(36, 36))
n_pitts <- length(pittsburgh)
chicago <- rep(c('seatbelt', 'none'), c(77, 52))
n_chicago <- length(chicago)

n_trials <- 10000
results <- numeric(n_trials)

for (i in 1:n_trials) {
    fake_pitts <- sample(pittsburgh, size=n_pitts, replace=TRUE)
    fake_chicago <- sample(chicago, size=n_chicago, replace=TRUE)
    fake_p_pitts <- sum(fake_pitts == 'seatbelt') / n_pitts
    fake_p_chicago <- sum(fake_chicago == 'seatbelt') / n_chicago
    fake_p_diff <- fake_p_pitts - fake_p_chicago
    results[i] <- fake_p_diff
}

hist(results, breaks=25,
    main='Bootstrap distribution of p differences',
    xlab='Bootstrap p differences')

p_limits <- quantile(results, c(0.025, 0.975))
rounded <- round(p_limits, 3)
message('95% percent limits for p differences: ', rounded[1], ' ', rounded[2])
```

:::
<!---
End of notebook.
-->

## Solution: [unemployment percentage @sec-exr-unemployment-percent]{#sec-soln-unemployment-percent}

In a sample of 200 people, 7 percent are found to be unemployed. Determine a 95
percent confidence interval for the true population proportion.

::: {.notebook name="unemployment_percent_solution" title="Unemployment percent solution"}

```{python}
import numpy as np
import matplotlib.pyplot as plt

rnd = np.random.default_rng()

n_trials = 10_000
results = np.zeros(n_trials)

for i in range(n_trials):
    fake_people = rnd.choice(['no job', 'job'], size=200, p=[0.07, 0.93])
    p_unemployed = np.sum(fake_people == 'no job') / 200
    results[i] = p_unemployed

plt.hist(results, bins=25)
plt.title('Bootstrap distribution p unemployed')
plt.xlabel('Bootstrap p unemployed')

p_limits = np.quantile(results, [0.025, 0.975])

print('95% percent limits for p differences:', p_limits)
```

```{r}
n_trials <- 10000
results <- numeric(n_trials)

for (i in 1:n_trials) {
    fake_people <- sample(c('no job', 'job'),
                          size=200,
                          replace=TRUE,
                          prob=c(0.07, 0.93))
    p_unemployed <- sum(fake_people == 'no job') / 200
    results[i] <- p_unemployed
}

hist(results, breaks=25,
     main='Bootstrap distribution p unemployed',
     xlab='Bootstrap p unemployed')

p_limits <- quantile(results, c(0.025, 0.975))
rounded <- round(p_limits, 3)
message('95% percent limits for p differences: ', rounded[1], ' ', rounded[2])
```

:::
<!---
End of notebook.
-->

## Solution: [battery lifetime @sec-exr-battery-lifetime]{#sec-soln-battery-lifetime}

> A sample of 20 batteries is tested, and the average lifetime is 28.85 months.
Establish a 95 percent confidence interval for the true average value. The
sample values (lifetimes in months) are listed below.

We use the "bootstrap" technique of drawing many bootstrap re-samples
with replacement from the original sample, and observing how the
re-sample means are distributed.

::: {.notebook name="battery_lifetime_solution" title="Battery lifetime solution"}

```{python}
import numpy as np
import matplotlib.pyplot as plt

rnd = np.random.default_rng()

lifetimes = np.array([30, 32, 31, 28, 31, 29, 29, 24, 30, 31,
                      28, 28, 32, 31, 24, 23, 31, 27, 27, 31])

print('Mean is:', np.mean(lifetimes))

n_lifetimes = len(lifetimes)
results = np.zeros(n_trials)

for i in range(n_trials):
    # Draw 20 lifetimes from "lifetimes, randomly and with replacement.
    fake_lifetimes = rnd.choice(lifetimes, size=n_lifetimes)
    # Find the average lifetime of the 20.
    fake_mean = np.mean(fake_lifetimes)
    # Keep score.
    results[i] = fake_mean

plt.hist(results, bins=25)
plt.title('Bootstrap distribution of mean battery lifetimes')
plt.xlabel('Bootstrap mean battery lifetime')

mean_limits = np.quantile(results, [0.025, 0.975])

print('95% percent limits for mean lifetimes:', mean_limits)
```

```{r}
lifetimes <- c(30, 32, 31, 28, 31, 29, 29, 24, 30, 31,
               28, 28, 32, 31, 24, 23, 31, 27, 27, 31)

message('Mean is: ', mean(lifetimes))

n_lifetimes <- length(lifetimes)
results <- numeric(n_trials)

for (i in 1:n_trials) {
    # Draw 20 lifetimes from "lifetimes, randomly and with replacement.
    fake_lifetimes <- sample(lifetimes, size=n_lifetimes, replace=TRUE)
    # Find the average lifetime of the 20.
    fake_mean <- mean(fake_lifetimes)
    # Keep score.
    results[i] <- fake_mean
}

hist(results, breaks=25,
    main='Bootstrap distribution of mean battery lifetimes',
    xlab='Bootstrap mean battery lifetime')

mean_limits <- quantile(results, c(0.025, 0.975))
rounded <- round(mean_limits, 2)
message('95% percent limits for mean lifetimes: ',
        rounded[1], ' ', rounded[2])
```

:::
<!---
End of notebook.
-->

## Solution: [optical density @sec-exr-optical-density]{#sec-soln-optical-density}

::: {.notebook name="optical_density_solution" title="Optical density solution"}

```{python}
import numpy as np
import matplotlib.pyplot as plt

rnd = np.random.default_rng()
```

Suppose we have 10 measurements of Optical Density on a batch of HIV
negative control samples:

```{python}
density = np.array(
    [.02, .026, .023, .017, .022, .019, .018, .018, .017, .022])
```

```{r}
density <- c(.02, .026, .023, .017, .022, .019, .018, .018, .017, .022)
```

Derive a 95 percent confidence interval for the sample mean. Are there
enough measurements to produce a satisfactory answer?

```{python}
n_density = len(density)

n_trials = 10_000
results = np.zeros(n_trials)

for i in range(n_trials):
    fake_density = rnd.choice(density, size=n_density)
    results[i] = np.mean(fake_density)

plt.hist(results, bins=25)
plt.title('Bootstrap distribution of density means')
plt.xlabel('Bootstrap density means')

mean_limits = np.quantile(results, [0.025, 0.975])

print('95% percent limits for density mean:', mean_limits)
```

```{r}
n_density <- length(density)

n_trials <- 10000
results <- numeric(n_trials)

for (i in 1:n_trials) {
    fake_density <- sample(density, size=n_density, replace=TRUE)
    results[i] <- mean(fake_density)
}

hist(results, breaks=25,
    main='Bootstrap distribution of density means',
    xlab='Bootstrap density means')

mean_limits <- quantile(results, c(0.025, 0.975))
rounded <- round(mean_limits, 3)
message('95% percent limits for density mean: ',
        rounded[1], ' ', rounded[2])
```

:::
<!---
End of notebook.
-->

## Solution: [voter participation @sec-exr-voter-participation] {#sec-soln-voter-participation}

The observed correlation coefficient between voter participation and spread is
moderate and negative. Is this more negative that what might occur by chance,
if no correlation exists in some underlying population, from which this sample
was taken?

1.  Create two groups of paper cards: 25 with participation rates, and
    25 with the spread values. Arrange the cards in pairs in accordance
    with the table, and compute the correlation coefficient between the
    shuffled participation and spread variables.
2.  Shuffle one of the sets, say that with participation, and compute
    correlation between shuffled participation and spread.
3.  Repeat step 2 many, say 1000, times. Compute the proportion of the
    trials in which correlation was at least as negative as that for the
    original data.

::: {.notebook name="voter_participation_solution" title="Voter participation in 1844 election"}

::: nb-only
Solution for voter participation exercise.
:::

```{python}
import numpy as np
import pandas as pd

rnd = np.random.default_rng()

voter_df = pd.read_csv('data/election_1844.csv')
participation = np.array(voter_df['Participation'])
spread = np.array(voter_df['Spread'])
```

```{r}
voter_df <- read.csv('data/election_1844.csv')
participation <- voter_df$Participation
spread <- voter_df$Spread
```


```{python}
# Compute correlation.  It's -0.425.
actual_r = np.corrcoef(participation, spread)[0, 1]
actual_r
```

```{r}
# Compute correlation.  It's -0.425.
actual_r <- cor(participation, spread)
actual_r
```

```{python}
n_trials = 10_000
results = np.zeros(n_trials)

for i in range(n_trials):
    # Shuffle the participation rates.
    shuffled = rnd.permuted(participation)
    # Compute re-sampled correlation.
    fake_r = np.corrcoef(shuffled, spread)[0, 1]
    # Keep the value in the results.
    results[i] = fake_r

plt.hist(results, bins=25)
plt.title('Distribution of shuffled correlations')
plt.xlabel('Correlation with shuffled participation')

# Count the trials when result <= observed.
k = np.sum(results <= actual_r)
# Compute the proportion of such trials.
kk = k / n_trials

print('Proportion of shuffled r <= observed:', np.round(kk, 2))
```

```{r}
n_trials <- 10000
results <- numeric(n_trials)

for (i in 1:n_trials) {
    # Shuffle the participation rates.
    shuffled <- sample(participation)
    # Compute re-sampled correlation.
    fake_r <- cor(shuffled, spread)
    # Keep the value in the results.
    results[i] <- fake_r
}

hist(results, breaks=25,
     main='Distribution of shuffled correlations',
     xlab='Correlation with shuffled participation')

# Count the trials when result <= observed.
k <- sum(results <= actual_r)
# Compute the proportion of such trials.
kk <- k / n_trials

message('Proportion of shuffled r <= observed: ', round(kk, 2))
```

:::
<!---
End of notebook
-->

From this we may conclude that the voter participation rates probably are
negatively related to the vote spread in the election. The actual value of the
correlation (-.425) cannot be explained by chance alone. In our Monte Carlo
simulation of the null-hypothesis a correlation that negative is found only
about 3 percent of the time.

See: @sec-soln-voter-participation.

## Solution: [association of runs and strikeouts @sec-exr-runs-strikeouts]{#sec-soln-runs-strikeouts}

We are looking at the correlation of home runs and strikeouts for major-league baseball players.

The instructions ask us to start here with the sum-of-products measure.

::: {.notebook name="homerun_sop_solution" title="Homeruns and strikeout sum of products."}

::: nb-only
Solution to exercise on relationship of home runs and strikeouts, using sum of
products.
:::

```{python}
import numpy as np

rnd = np.random.default_rng()

homeruns = np.array([14, 20, 0, 38, 9, 38, 22, 31, 33,
                     11, 40, 5, 15, 32, 3, 29, 5, 32])
strikeout = np.array([135, 153, 120, 161, 138, 175, 126, 200, 205,
                      147, 165, 124, 169, 156, 36, 98, 82, 131])
```

```{r}
homeruns <- c(14, 20, 0, 38, 9, 38, 22, 31, 33,
              11, 40, 5, 15, 32, 3, 29, 5, 32)
strikeout <- c(135, 153, 120, 161, 138, 175, 126, 200, 205,
               147, 165, 124, 169, 156, 36, 98, 82, 131)

```

```{python}
# The sum of products approach.
actual_sop = np.sum(homeruns * strikeout)

n_trials = 10_000
results = np.zeros(n_trials)

for i in range(n_trials):
    shuffled_runs = rnd.permuted(homeruns)
    fake_sop = np.sum(shuffled_runs * strikeout)
    results[i] = fake_sop

plt.hist(results, bins=25)
plt.title('Distribution of shuffled sum of products')
plt.xlabel('Sum of products for shuffled homeruns')

k = np.sum(results >= actual_sop)
kk = k / n_trials

print('p shuffled sum of products >= actual:', np.round(kk, 3))
```

```{r}
# The sum of products approach.
actual_sop <- sum(homeruns * strikeout)

n_trials <- 10000
results <- numeric(n_trials)

for (i in 1:n_trials) {
    shuffled_runs <- sample(homeruns)
    fake_sop <- sum(shuffled_runs * strikeout)
    results[i] <- fake_sop
}

hist(results, breaks=25,
     main='Distribution of shuffled sum of products',
     xlab='Sum of products for shuffled homeruns')

k <- sum(results >= actual_sop)
kk <- k / n_trials

message('p shuffled sum of products >= actual: ', round(kk, 3))
```

Interpretation: In 10,000 simulations, random shuffling very rarely produced
a value as high as observed. Therefore, we conclude that random chance could
not reasonably be responsible for the observed degree of correlation.

:::
<!---
End of notebook.
-->

## Solution: [runs, strikeouts and correlation coefficient @sec-exr-runs-strikeouts-r]{#sec-soln-runs-strikeouts-r}

Again, we are looking at the correlation of home runs and strikeouts for
major-league baseball players.  This time we will use the correlation
coefficient ($r$) measure.

::: {.notebook name="homerun_correlation_solution" title="Homeruns and strikeout correlation"}

::: nb-only
Solution for exercise on relationship of home runs and strikeouts, using correlation coefficient.
:::

```{python}
import numpy as np

rnd = np.random.default_rng()

homeruns = np.array([14, 20, 0, 38, 9, 38, 22, 31, 33,
                     11, 40, 5, 15, 32, 3, 29, 5, 32])
strikeout = np.array([135, 153, 120, 161, 138, 175, 126, 200, 205,
                      147, 165, 124, 169, 156, 36, 98, 82, 131])
```

```{r}
homeruns <- c(14, 20, 0, 38, 9, 38, 22, 31, 33,
              11, 40, 5, 15, 32, 3, 29, 5, 32)
strikeout <- c(135, 153, 120, 161, 138, 175, 126, 200, 205,
               147, 165, 124, 169, 156, 36, 98, 82, 131)

```

```{python}
# The correlation approach.
actual_r = np.corrcoef(homeruns, strikeout)[0, 1]

n_trials = 10_000
results = np.zeros(n_trials)

for i in range(n_trials):
    shuffled_runs = rnd.permuted(homeruns)
    fake_r = np.corrcoef(shuffled_runs, strikeout)[0, 1]
    results[i] = fake_r

plt.hist(results, bins=25)
plt.title('Distribution of shuffled r')
plt.xlabel('r for shuffled homeruns')

k = np.sum(results >= actual_r)
kk = k / n_trials

print('p shuffled r >= actual:', np.round(kk, 3))
```

```{r}
# The correlation approach.
actual_r <- cor(homeruns, strikeout)

n_trials <- 10000
results <- numeric(n_trials)

for (i in 1:n_trials) {
    shuffled_runs <- sample(homeruns)
    fake_r <- cor(shuffled_runs, strikeout)
    results[i] <- fake_r
}

hist(results, breaks=25,
    main='Distribution of shuffled r',
    xlab='r for shuffled homeruns')

k <- sum(results >= actual_r)
kk <- k / n_trials

message('p shuffled r >= actual: ', round(kk, 3))
```

Interpretation: a correlation coefficient as high as the observed value
(.62) occurred only about 0.1% of the time by chance. Hence, provisionally, we
choose to reject chance as an explanation for such a high value of the
correlation coefficient.

Notice, we get the same answer for correlation coefficients as we do for sum of
products. In fact, correlation coefficients must give us the same answer (apart
from random variation from the permutation), as the tests of association are
equivalent when we compare between different orderings of the same sequences.

:::
<!---
End of notebook.
-->

### Solution: [money and exchange rate @sec-exr-money-exchange]{#sec-soln-money-exchange}

::: {.notebook name="exchange_rates_solution" title="Exchange rates and money supply"}

::: nb-only
Solution notebook for exercise on exchange rates and money supply.
:::

```{python}
import numpy as np
import pandas as pd

rnd = np.random.default_rng()

exchange_df = pd.read_csv('data/exchange_rates.csv')
exchange_rates = np.array(exchange_df['exchange_rate'])
money_supply = np.array(exchange_df['money_supply'])
```

```{r}
exchange_df <- read.csv('data/exchange_rates.csv')
exchange_rates <- exchange_df$exchange_rate
money_supply <- exchange_df$money_supply
```

```{python}
# Correlation.
actual_r = np.corrcoef(exchange_rates, money_supply)[0, 1]

n_trials = 10_000
results = np.zeros(n_trials)

for i in range(n_trials):
    shuffled_rates = rnd.permuted(exchange_rates)
    fake_r = np.corrcoef(shuffled_rates, money_supply)[0, 1]
    results[i] = fake_r

plt.hist(results, bins=25)
plt.title('Distribution of shuffled exchange rates r values')
plt.xlabel('r for shuffled exchange rate')

k = np.sum(results >= actual_r)
kk = k / n_trials

print('p shuffled r >= actual:', np.round(kk, 3))
```

```{r}
# Correlation.
actual_r <- cor(exchange_rates, money_supply)

n_trials <- 10000
results <- numeric(n_trials)

for (i in 1:n_trials) {
    shuffled_rates <- sample(exchange_rates)
    fake_r <- cor(shuffled_rates, money_supply)
    results[i] <- fake_r
}

hist(results, breaks=25,
     main='Distribution of shuffled exchange rates r values',
     xlab='r for shuffled exchange rate')

k <- sum(results >= actual_r)
kk <- k / n_trials

message('p shuffled r >= actual: ', round(kk, 3))
```

:::
<!---
End of notebook.
-->

Result: prob = .001

Interpretation: The observed correlation (.419) between the exchange rate and
the money supply is seldom exceeded by random experiments with these data.
Thus, the observed result 0.419 cannot be reasonably explained by chance alone
and we conclude that it is statistically surprising.
